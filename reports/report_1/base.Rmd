---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 1: Group 64"
author: "Jakob Gerhard Martinussen"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document
  #pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "hold")
knitr::opts_chunk$set(error = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

# Problem 1: Multiple linear regression 

```{r,echo=TRUE,eval=TRUE}
library(GLMsData)
data("lungcap")
lungcap$Htcm <- lungcap$Ht * 2.54
modelA <- lm(
  log(FEV) ~ Age + Htcm + Gender + Smoke,
  data=lungcap
)
summary(modelA)
```

**Q1:** 

The equation for the fitted model is

$$
\log(Y_{\text{FEV}})
= f(x) + \epsilon
= x^T \beta + \epsilon
=
\beta_0 +
\beta_{\text{age}}x_{\text{age}} +
\beta_{\text{height}}x_{\text{height}} +
\beta_{\text{male}}x_{\text{male}} +
\beta_{\text{smoke}}x_{\text{smoke}} +
\epsilon
$$

**Q2:** 
    
#### `Estimate`

The `Estimate` column represents the entries in the $\hat{\beta}$ vector of length 5, the model *estimation* for $\beta$.
Remember that the model is fitted with a *logarithmic* FEV response, so the model prediction of an induvidual $x$ becomes

$$
\hat{Y} = e^{x^T \hat{\beta}}.
$$

The `(Intercept)` estimate represents a non-smoking female individual of age 0 and height 0, which is nonsensical.

```{r}
beta <- coefficients(modelA)
interceptFEV <- exp(beta["(Intercept)"])
interceptFEV
```

Such an imaginary individual is predicted to have a `FEV` of $\approx 0.14$ litres.
Now, in order to explain the remaining estimates, assume one of the covariates, $x_j$, to change to $x_j + 1$.
How does the individual prediction change from $\hat{Y}_{\text{before}}$ to $\hat{Y}_{\text{after}}$?

$$
\hat{Y}_{after}
= e^{x^T \beta}
= e^{... + (x_j + 1) \beta_j + ...}
= e^{\beta_j} e^{x^T \beta}
= e^{\beta_j} \hat{Y}_{before}
$$

The exponential of the estimates represents the *multiplicative change* of the model prediction as the respective covariate changes.
In our case, these multiplicative effects are

```{r}
library(tidyverse)
library(kableExtra)
enframe(exp(beta)) %>% kable()
```

Here we see that an individual which smokes is expected to have $\approx 95.5\%$ of the `FEV` of a non-smoking individual, everything else being equal. Likewise, for every additional year of age, an individual is expected to increase their `FEV` by $\approx 2.7\%$.


#### `Std.Error`

The `Std.Error` column shows the model estimate of the standard error, $\mathrm{SE}({\beta_i})$.
In order to calculate these estimates, we need the notion of \textit{residuals}, defined by

$$
e_i := Y_i - \hat{Y}_i = Y_i - x_i^T \hat{\beta},
$$

i.e. the difference between the model predictions and the actually observed values for the input data set.
The population distribution variance parameter can now be estimated by

$$
\hat{\sigma}^2 = \frac{\sum_{i = 1}^n e_i^2}{n - p - 1} = \frac{\text{RSS}}{n - p - 1},
$$

where RSS is the residual sum of squares.
The estimation of the standard error of the parameter estimation for $\beta_j$ can now be expressed as 

$$
\hat{\mathrm{SE}}(\hat{\beta}_i) = \sqrt{[(X^TX)^{-1}]_{[i,i]}} \hat{\sigma} := \sqrt{c_{jj}} \hat{\sigma},
$$

where $c_jj$ is the j'th diagonal entry of the matrix $(X^T X)^{-1}$.
For instance, $\beta_{\mathrm{age}}$ is estimated to be normally distributed with standard error

```{r}
modelASummary <- summary(modelA)
modelAEstimates <- as_tibble(modelASummary$coefficients)
modelAEstimates$`Std. Error`[2]
```


#### `Residual standard error`

The residual standard error (RSS) for our model is

```{r}
modelASummary$sigma
```

and is found by summing the square difference between the observed values and the model predictions, as follows

$$
\mathrm{RSS} =
\sum_{i = 1}^{n} e_i^2 =
\sum_{i = 1}^{n} (Y_i - \hat{Y_i})^2 =
\sum_{i = 1}^{n} (Y_i - x_i^T \hat{\beta})^2,
$$

and can be thought of as a summary of how much the model prediction "misses" the input data.

#### `F-statistic`

The `F-statistic` is a test statistic for the following hypothesis test

$$
\mathrm{H_0}: \beta = \vec{0} \\
\text{ vs. } \\
\mathrm{H_1}: \beta_j \neq 0, \text{for at least one } j.
$$

It tests if at least one of the model covariates have explanatory power. The test statistic is constructed as follows

$$
F = \frac{(\mathrm{TSS} - \mathrm{RSS}) / p}{\mathrm{RSS} / (n - p - 1)}
\sim F_{p, n - p - 1},
$$

where

$$
\mathrm{TSS} := \sum_{i = 1}^n (y_i - \bar{y})^2 \\
\bar{y} := \frac{1}{n} \sum_{i = 1}^n y_i.
$$

The test statistic is Fisher distributed with $p$ and $n - p - 1$ degrees of freedom, in our case $4$ and $649$ respectively. In our case, this test results in a $p$-value of less than $2.2 \cdot 10^{-16}$, and we can relatively confidently conclude that the regression is significant.

**Q3:** 

The proportion of variability explained by `modelA` can be found by calculating the multiple R-squared statistic, given by

$$
R^2 := \frac{\mathrm{TSS} - \mathrm{RSS}}{\mathrm{TSS}}
= 1 - \frac{RSS}{TSS} \in [0, 1]
$$
In our case, this value is $\approx 80.95\%$.

**Q4:**


Here we use \textit{standardized residual plots} in order to check our model assumptions.
One of our assumptions is

$$
\epsilon \sim \mathrm{N}_n(0, \sigma^2 I).
$$

With other words, the error terms are identically, and independently normally distributed with zero expected value. We can use the model resiuduals as estimators for the error terms, and check these assumptions. First,

```{r,eval=TRUE}
library(ggplot2)
# Residuals vs fitted
ggplot(modelA, aes(.fitted, .stdresid)) +
  geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(
    x = "Fitted values",
    y = "Standardized residuals",
    title = "Fitted values vs. Standardized residuals",
    subtitle = deparse(modelA$call)
  )
```

Here we expect the standardized residuals to be symmetrically distributed across the $x$-axis (zero expected), and no discernible magnitude trend along the $x$-axis. The latter requirement follows from the independence between the $(Y_i, x_i)$ and $\epsilon_i$. Both requirements are satisfies, so no issues so far. Next,

```{r}
# qq-plot of residuals
ggplot(modelA, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(
    intercept = 0,
    slope = 1,
    linetype = "dotted"
  ) +
  labs(
    x = "Theoretical quantiles",
    y = "Standardized residuals",
    title = "Normal Q-Q",
    subtitle = deparse(modelA$call)
  )
```

Here we plot a normal Q-Q plot for the model residuals. Ideally, we would like to see all points lying perfectly along a line going through the origin.
In this case, we have some deviation from this straight line at the tails. This indicates a somewhat heavy left tail and light right tail, but it is still within acceptable limits. No reason to conclude non-normality of error terms.

Finally,

```{r}
# normality test
library(nortest) 
ad.test(rstudent(modelA))
```

Here, the "Anderson-Darling normality test" is performed on the *studentized residuals*. Due to a large value for A, we must reject the null-hypothesis of residual normality. The conclusions based on the plots above are therefore apparently wrong according to this test.

**Q5:** 

We now fit a new model, `modelB`, identical to `modelA` except for not using a *logarithmic* transformation for `FEV`.

```{r,eval=TRUE}
modelB <- lm(FEV ~ Age + Htcm + Gender + Smoke, data=lungcap)
summary(modelB)
```

Since we have the exact same covariates, we can assess the two models by comparing their $R^2$ statistics. Since $R_B^2 < R_A^2$, `modelA` is to be preferred if explanatory power is important.

Arguably, we should still choose `modelB` if it satisfies the error term normality condition, since this is an important assumption of the linear regression model.

```{r}
ad.test(rstudent(modelB))
```

We still can't conclude that error normality is definitely satisfied here, so we still choose `modelA` over `modelB`.

**Q6:** 

$$
\mathrm{H_0}: \beta_{\mathrm{age}} = 0 \\
\text{ vs. } \\
\mathrm{H_1}: \beta_{\mathrm{age}} \neq 0
$$

$$
T_{\mathrm{age}} = \frac{\hat{\beta}_{\mathrm{age}} - \beta_{\mathrm{age}}}{\sqrt{\hat{\mathrm{Var}}(\hat{\beta}_{\mathrm{age}})}} \sim t_{n - 2}
$$
$$
p\text{-value} = P(T_{\mathrm{age}} > t_{\mathrm{age}})
$$

```{r,eval=TRUE}
n <- nobs(modelA)
betaAge <- beta["Age"]
betaSE <- modelAEstimates$`Std. Error`[2]
tTestStatistic <- betaAge / betaSE
pValue <- 2 * pt(q = tTestStatistic, df = n - 2, lower.tail = FALSE)
pValue
```

The $p$-value is $\approx 7.1 \cdot 10^{-12}$, which coincides with the model summary output from earlier.

With a significance level of 5%, the critical value (which gives the cut-off) can be compared
with our $t$-statistic.

```{r}
alpha <- 0.05
criticalValue <- qt(p = alpha / 2, df = n - 2, lower.tail = FALSE)
print(list(tStatistic = tTestStatistic, criticalValue = criticalValue))
```

We can reject the null-hypothesis and conclude that $\beta_{\mathrm{age}} \neq 0$.

**Q7:** 

$$
P(-t_{\alpha / 2, n - 2} < T_{\mathrm{age}} < t_{\alpha / 2, n - 2}) = 1 - \alpha \\
\implies
P(-t_{\alpha / 2, n - 2} < \frac{\hat{\beta}_{\mathrm{age}} - \beta_{\mathrm{age}}}{\hat{\mathrm{SE}}(\hat{\beta}_{\mathrm{age}})} < t_{\alpha / 2, n - 2}) = 1 - \alpha \\
\implies
P(\hat{\beta}_{\mathrm{age}} - t_{\alpha / 2, n - 2} \cdot \hat{\mathrm{SE}}(\hat{\beta}_{\mathrm{age}}) < \beta_{\mathrm{age}} < \hat{\beta}_{\mathrm{age}} + t_{\alpha / 2, n - 2} \cdot \hat{\mathrm{SE}}(\hat{\beta}_{\mathrm{age}})) = 1 - \alpha
$$

So we must find the interval $\hat{\beta}_{\mathrm{age}} \pm t_{\alpha / 2, n - 2} \cdot \hat{\mathrm{SE}}(\hat{\beta}_{\mathrm{age}})$.

```{r,eval=TRUE}
alpha <- 0.05
radius <- qt(p = alpha / 2, df = n - 2, lower.tail = FALSE) * betaSE
confidenceInterval <- tibble(lower = betaAge - radius, upper = betaAge + radius)
confidenceInterval %>% kable()
```

Notice that the 95% confidence interval for $\beta_{\mathrm{age}}$ is entirely positive.
We can therefore relatively conclude that `age` has a positive effect on `FEV`, as the multiplicative effect becomes greater than one. This is really useful.
This is positive 95% interval is also closely related to the fact that we rejected the null-hypothesis at a 5% $p$-value cut-off in the previous task,
since we can say with 95% confidence that $\beta_{\mathrm{age}}$ is nonzero.

**Q8:**

We now want to make a model prediction for a 16 year old non-smoking male of height 170 centimeters, $x_0$.

First we want to predict a value for `log(FEV)` by calculating

$$
\hat{\log({\mathrm{FEV}})} = x_0^T \hat{\beta}
$$
This can be done with the R-function `predict()` like this 

```{r,eval=TRUE}
new = data.frame(Age=16, Htcm=170, Gender="M", Smoke=0)
logPrediction <- as_tibble(
  predict(
    modelA,
    newdata = new,
    interval = "predict",
    type = "response",
    level = 0.95
  )
) %>%
  rename(
    lower = lwr,
    upper = upr
  )
print(logPrediction$fit)
```

Our best guess, based on `modelA`, for `log(FEV)` is thus $\approx 1.33$.

We can also construct a confidence interval for this predicted logarithmic value by using the following formula (given without proof)

$$
[{\bf x}_0^T \hat{\beta}-t_{\alpha/2,n-p-1}\hat{\sigma}\sqrt{1+{\bf x}_0^T({\bf X}^T{\bf X})^{-1}{\bf x}_0},~{\bf x}_0^T \hat{\beta}+t_{\alpha/2,n-p-1}\hat{\sigma}\sqrt{1+{\bf x}_0^T({\bf X}^T{\bf X})^{-1}{\bf x}_0}],
$$.

and exponentially transform the interval in order to get a 95% prediction interval for `FEV`.
We can use `predict()` for this purpose as well

```{r}
logInterval <- logPrediction[c("lower", "upper")]
FEVInterval <- exp(logInterval)
FEVInterval %>% kable()
```

Our 95% prediction interval is $[2.82, 5.01]$, which is a quite wide interval.
If we compare this interval to our `FEV` observations

```{r}
summary(lungcap$FEV)
```

the most useful thing we can conclude is that the individual likely has an above average `FEV`.

# Problem 2: Classification 

```{r}
library(class)# for function knn
library(caret)# for confusion matrices

raw = read.csv("https://www.math.ntnu.no/emner/TMA4268/2019v/data/tennis.csv")
M = na.omit(
  data.frame(
    y = as.factor(raw$Result),
    x1 = raw$ACE.1 - raw$UFE.1 - raw$DBF.1,
    x2 = raw$ACE.2 - raw$UFE.2 - raw$DBF.2
  )
)
set.seed(4268) # for reproducibility
tr <- sample.int(nrow(M), nrow(M) / 2)
trte <- rep(1, nrow(M))
trte[tr] <- 0
Mdf <- data.frame(M, "istest" = as.factor(trte))
```

**Q9:** 

Define $\mathcal{N}_x$ to be the set of the $K$ closest points to the point $x = (x_1, x_2)$.
The KNN istimator $\hat{y}(x) \in \{0, 1\}$ is determined by taking a "majority vote" of
its $N$ closest neighbors.
The KNN estimate of the posterior class probability becomes

$$
\hat{P}(Y = y | X = x)
=
\frac{1}{K} \sum_{i \in \mathcal{N_x}} I(y_i = y)
=
\begin{cases}
  \frac{1}{K} \sum_{i \in \mathcal{N_x}} y_i,&\text{if } &y = 1 \\
  1 - \frac{1}{K} \sum_{i \in \mathcal{N_x}} y_i,&\text{if } &y = 0
\end{cases}
$$

And the estimator can bayes classifier can be used to construct the estimator $\hat{y}(x)$

$$
\hat{y}(x) = 
\begin{cases} 
  1, ~~~~~\text{if } \hat{P}(Y = 1 | X = x) \geq 0.5 \\
  0, ~~~~~\text{otherwise}
\end{cases}
$$

**Q10:** 

We now calculate the error rates of the training and test sets for $K = 1, 2, ..., 30$ and plot these results.

```{r,eval=TRUE}
dataset <- as_tibble(Mdf)
train <- dataset %>% filter(istest == "0") %>% select(x1, x2)
test <- dataset %>% filter(istest == "1") %>% select(x1, x2)

trainAnswers <- dataset %>% filter(istest == "0") %>% pull(y)
testAnswers <- dataset %>% filter(istest == "1") %>% pull(y)

train.e <- vector()
test.e <- vector()

knnErrorRates <- function(train, test, trainAnswers, testAnswers) {
  errorRates <- vector()
  for (k in 1:30) {
    testPredictions <- class::knn(
      train = train,
      test = test,
      cl = trainAnswers,
      k = k
    )
    errorRates <- append(
      errorRates,
      mean(testPredictions != testAnswers)
    )
  }
  return(errorRates)
}
test.e <- knnErrorRates(
  train = train,
  test = test,
  trainAnswers = trainAnswers,
  testAnswers = testAnswers
)
train.e <- knnErrorRates(
  train = train,
  test = train,
  trainAnswers = trainAnswers,
  testAnswers = trainAnswers
)
result <- tibble(
  K = 1:30,
  testError = test.e,
  trainError = train.e
)

result %>% ggplot(aes(x = K)) +
  geom_line(aes(y = testError, col = "Test set")) +
  geom_line(aes(y = trainError, col = "Training set")) +
  ylab("Error rate")
```

As expected, the training set has the lowest error rate for $K = 1$. The reason for the error not being
exactly zero at $K = 1$ is due to duplicate $x$ entries.

```{r, eval=FALSE}
set.seed(0)
ks <- 1:30 # Choose K from 1 to 30.
idx <- createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop. 
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv", 
# where folds are rows and values of $K$ are columns.
cv = sapply(
  ks,
  function(k){ 
    sapply(
      seq_along(idx),
      function(j) {
        yhat <- class::knn(
          train = M[tr[ -idx[[j]] ], -1],
          cl = M[tr[ -idx[[j]] ], 1],
          test = M[tr[ idx[[j]] ], -1],
          k = k
        )
        mean(M[tr[ idx[[j]] ], 1] != yhat)
      }
    )
  }
)
```


**Q11:** 

```{r, eval=FALSE}
cv.e <- colMeans(cv)
cv.se <- apply(cv, 2, sd) / sqrt(5)
k.min <- which.min(cv.e)
```

**Q12:** 

We now plot the misclassification errors.

```{r,eval=FALSE}
library(colorspace)
co <- rainbow_hcl(3)
par(mar = c(4,4,1,1) + 0.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type = "o", pch = 16, ylim = c(0, 0.7), col = co[2],
     xlab = "Number of neighbors", ylab = "Misclassification error")
arrows(ks, cv.e-cv.se, ks, cv.e+cv.se, angle = 90, length = .03,
       code = 3, col = co[2])
lines(ks, train.e, type = "o", pch = 16, ylim = c(0.5, 0.7), col = co[3])
lines(ks, test.e, type = "o", pch = 16, ylim = c(0.5, 0.7), col = co[1])
legend("topright", legend = c("Test", "5-fold CV", "Training"), lty = 1,
       col = co)
```

The bias of $\hat{y}(x)$ will increase with increasing $K$, but the variance will *decrease* with increasing $K$.
The variance is high for $K = 1$ because only the nearest neighbours is used and noise will therefore affect the model quite a lot.
The bias is low for $K = 1$ because the model will be very close to the training data.

**Q13:** 

```{r,eval=FALSE}
k <- tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
size <- 100
xnew <- apply(M[tr,-1], 2, function(X) seq(min(X), max(X), length.out = size))
grid <- expand.grid(xnew[,1], xnew[,2])
grid.yhat <- knn(M[tr,-1], M[tr,1], k=k, test=grid)
np <- 300
par(mar = rep(2,4), mgp = c(1, 1, 0))
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels = .5, 
        xlab = expression("x"[1]), ylab = expression("x"[2]), axes = FALSE,
        main = paste0(k,"-nearest neighbors"), cex = 1.2, labels = "")
points(grid, pch = ".", cex = 1, col = grid.yhat)
points(M[1:np,-1], col = factor(M[1:np,1]), pch = 1, lwd = 1.5)
legend("topleft", c("Player 1 wins", "Player 2 wins"), 
       col = c("red", "black"), pch = 1)
box()
```


**Q14:** 

```{r,eval=FALSE}
K=# your choice from Q13
  
# knn with prob=TRUE outputs the probability of the winning class
# therefore we have to do an extra step to get the probability of player 1 winning
KNNclass <- class::knn(train = M[tr,-1], cl = M[tr,1], test = M[-tr,-1], k = K,prob=TRUE)
KNNprobwinning <- attributes(KNNclass)$prob
KNNprob <- ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)
# now KNNprob has probability that player 1 wins, for all matches in the test set

library(pROC)
# now you use predictor=KNNprob and response=M[-tr,1] 
# in your call to the function roc in the pROC library
```

**Q15:**

```{r,eval=TRUE}
# here you write your code
```


# Problem 3: Bias-variance trade-off 

Here you see how to write formulas with latex (needed below)
$$
\hat{\boldsymbol \beta}=({\bf X}^T{\bf X})^{-1}{\bf X}^T{\bf Y}
$$

**Q16:** 

**Q17:** 

**Q18:** 
$$\text{E}[(Y_0-\hat{f}({\bf x}_0))^2]=[\text{E}(\hat{f}({\bf x}_0)-f({\bf x}_0)]^2+\text{Var}(\hat{f}({\bf x}_0) ) + \text{Var}(\varepsilon)$$

Ridge estimator:
$$
\widetilde{\boldsymbol \beta}=({\bf X}^T{\bf X}+\lambda {\bf I})^{-1}{\bf X}^T{\bf Y}
$$

**Q19:** 

**Q20:** 

**Q21:** 
$$\text{E}[(Y_0-\widetilde{f}({\bf x}_0))^2]=[\text{E}(\widetilde{f}({\bf x}_0)-f({\bf x}_0)]^2+\text{Var}(\widetilde{f}({\bf x}_0) ) + \text{Var}(\varepsilon)$$


```{r}
values <- dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/BVtradeoffvalues.dd")
X <- values$X
dim(X)
x0 <- values$x0
dim(x0)
beta <- values$beta
dim(beta)
sigma <- values$sigma
sigma
```

Hint: we perform matrix multiplication using `%*%`, transpose of a matrix `A` with `t(A)` and inverse with `solve(A)`. 

**Q22:** 

```{r,eval=FALSE}
sqbias <- function(lambda, X, x0, beta) {
  p <- dim(X)[2]
  value <- #HERE YOU FILL IN
  return(value)
}
thislambda <- seq(0,2,length=500)
sqbiaslambda <- rep(NA,length(thislambda))
for (i in 1:length(thislambda)) {
  sqbiaslambda[i] <- sqbias(thislambda[i],X,x0,beta)
}
plot(thislambda, sqbiaslambda, col = 2, type = "l")
```

**Q23:** 

```{r,eval=FALSE}
variance <- function(lambda, X, x0, sigma) {
  p <- dim(X)[2]
  inv <- solve(t(X)%*%X+lambda*diag(p))
  value <- #HERE YOU FILL IN
  return(value)
}
thislambda <- seq(0,2,length=500)
variancelambda <- rep(NA,length(thislambda))
for (i in 1:length(thislambda)) {
  variancelambda[i] <- variance(thislambda[i], X, x0, sigma)
}
plot(thislambda, variancelambda, col = 4, type = "l")
```


**Q24:** 

```{r,eval=FALSE}
tot <- sqbiaslambda+variancelambda+sigma^2
which.min(tot)
thislambda[which.min(tot)]
plot(thislambda, tot, col = 1, type = "l", ylim = c(0,max(tot)))
lines(thislambda, sqbiaslambda, col = 2)
lines(thislambda, variancelambda, col = 4)
lines(thislambda, rep(sigma^2,500), col = "orange")
abline(v = thislambda[which.min(tot)], col = 3)
```
