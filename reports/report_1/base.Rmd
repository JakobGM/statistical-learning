---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 1: Group 64"
author: "Jakob Gerhard Martinussen, Alm Wilson"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document
  #pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "hold")
knitr::opts_chunk$set(error = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

# Problem 1: Multiple linear regression 

```{r,echo=TRUE,eval=TRUE}
library(GLMsData)
data("lungcap")
lungcap$Htcm <- lungcap$Ht * 2.54
modelA <- lm(
  log(FEV) ~ Age + Htcm + Gender + Smoke,
  data=lungcap
)
summary(modelA)
```

**Q1:** 

The equation for the fitted model is

$$
\log(\text{FEV})
= Y
= f(x) + \epsilon
= x^T \beta + \epsilon
=
\beta_0 +
\beta_{\text{age}}x_{\text{age}} +
\beta_{\text{height}}x_{\text{height}} +
\beta_{\text{male}}x_{\text{male}} +
\beta_{\text{smoke}}x_{\text{smoke}} +
\epsilon
$$

**Q2:** 
    
#### `Estimate`

The `Estimate` column represents the entries in the $\hat{\beta}$ vector of length 5, the model *estimation* for $\beta$.
Remember that the model is fitted with a *logarithmic* FEV response, so the model prediction of an induvidual $x$'s FEV becomes

$$
\text{FEV} = e^{\hat{Y}} = e^{x^T \hat{\beta}}.
$$

The `(Intercept)` estimate represents a non-smoking female individual of age 0 and height 0, which is nonsensical.

```{r}
beta <- coefficients(modelA)
interceptFEV <- exp(beta["(Intercept)"])
interceptFEV
```

Such an imaginary individual is predicted to have a `FEV` of $\approx 0.14$ litres.
Now, in order to explain the remaining estimates, assume one of the covariates, $x_j$, to change to $x_j + 1$.
How does the model's FEV prediction change from $\text{FEV}_{\text{before}}$ to $\text{FEV}_{\text{after}}$ in such a case?

$$
\text{FEV}_{after}
= e^{x^T \hat\beta}
= e^{... + (x_j + 1) \hat\beta_j + ...}
= e^{\hat \beta_j} e^{x^T \hat \beta}
= e^{\hat \beta_j} \text{FEV}_{before}
$$

The exponential of the estimates represents the *multiplicative change* of the model prediction as the respective covariate changes.
In our case, these multiplicative effects are

```{r}
library(tidyverse)
library(kableExtra)
enframe(exp(beta)) %>% kable()
```

Here we see that an individual which smokes is expected to have $\approx 95.5\%$ of the `FEV` of a non-smoking individual, everything else being equal. Likewise, for every additional year of age, an individual is expected to increase their `FEV` by $\approx 2.7\%$.


#### `Std.Error`

The `Std.Error` column shows the model estimate of the standard error, $\mathrm{SE}({\beta_i})$.
In order to calculate these estimates, we need the notion of *residuals*, defined by

$$
e_i := Y_i - \hat{Y}_i = Y_i - x_i^T \hat{\beta},
$$

i.e. the difference between the model predictions and the actually observed values for the input data set.
The population distribution variance parameter can now be estimated by

$$
\hat{\sigma}^2 = \frac{\sum_{i = 1}^n e_i^2}{n - p - 1} = \frac{\text{RSS}}{n - p - 1},
$$

where RSS is the residual sum of squares.
The estimation of the standard error of the parameter estimation for $\beta_j$ can now be expressed as 

$$
\hat{\mathrm{SE}}(\hat{\beta}_i) = \sqrt{[(X^TX)^{-1}]_{[i,i]}} \hat{\sigma} := \sqrt{c_{jj}} \hat{\sigma},
$$

where $c_{jj}$ is the j'th diagonal entry of the matrix $(X^T X)^{-1}$.
For instance, $\beta_{\mathrm{age}}$ is estimated to be normally distributed with standard error

```{r}
modelASummary <- summary(modelA)
modelAEstimates <- as_tibble(modelASummary$coefficients)
modelAEstimates$`Std. Error`[2]
```


#### `Residual standard error`

The residual standard error (RSS) for our model is

```{r}
modelASummary$sigma
```

and is found by summing the square difference between the observed values and the model predictions, as follows

$$
\mathrm{RSS} =
\sum_{i = 1}^{n} e_i^2 =
\sum_{i = 1}^{n} (Y_i - \hat{Y_i})^2 =
\sum_{i = 1}^{n} (Y_i - x_i^T \hat{\beta})^2,
$$

and can be thought of as a summary of how much the model prediction "misses" the input data.

#### `F-statistic`

The `F-statistic` is a test statistic for the following hypothesis test

\begin{gather*}
\mathrm{H_0}: \beta = \vec{0} \\
\text{ vs. } \\
\mathrm{H_1}: \beta_j \neq 0, \text{for at least one } j.
\end{gather*}

It tests if at least one of the model covariates have explanatory power. The test statistic is constructed as follows

$$
F = \frac{(\mathrm{TSS} - \mathrm{RSS}) / p}{\mathrm{RSS} / (n - p - 1)}
\sim F_{p, n - p - 1},
$$

where

\begin{gather*}
\mathrm{TSS} := \sum_{i = 1}^n (y_i - \bar{y})^2 \\
\bar{y} := \frac{1}{n} \sum_{i = 1}^n y_i.
\end{gather*}

The test statistic is Fisher distributed with $p$ and $n - p - 1$ degrees of freedom, in our case $4$ and $649$ respectively. This test results in a $p$-value of less than $2.2 \cdot 10^{-16}$, and we can relatively confidently conclude that the regression is significant.

**Q3:** 

The proportion of variability explained by `modelA` can be found by calculating the multiple R-squared statistic, given by

$$
R^2 := \frac{\mathrm{TSS} - \mathrm{RSS}}{\mathrm{TSS}}
= 1 - \frac{RSS}{TSS} \in [0, 1]
$$
In our case, this value is $\approx 80.95\%$.

**Q4:**


Here we use *standardized residual plots* in order to check our model assumptions.
One of our assumptions is

$$
\epsilon \sim \mathrm{N}_n(0, \sigma^2 I).
$$

With other words, the error terms are identically, and independently normally distributed with mean zero. We can use the model resiuduals as estimators for the error terms, and check these assumptions. First,

```{r,eval=TRUE}
library(ggplot2)
# Residuals vs fitted
ggplot(modelA, aes(.fitted, .stdresid)) +
  geom_point(pch = 21) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
  labs(
    x = "Fitted values",
    y = "Standardized residuals",
    title = "Fitted values vs. Standardized residuals",
    subtitle = deparse(modelA$call)
  )
```

Here we expect the standardized residuals to be symmetrically distributed across the $x$-axis (zero expected), and no discernible magnitude trend along the $x$-axis. The latter requirement follows from the independence between the observational pairs $(Y_i, x_i)$ and the error terms $\epsilon_i$. Both requirements are satisfied, so no issues so far. Next,

```{r}
# qq-plot of residuals
ggplot(modelA, aes(sample = .stdresid)) +
  stat_qq(pch = 19) + 
  geom_abline(
    intercept = 0,
    slope = 1,
    linetype = "dotted"
  ) +
  labs(
    x = "Theoretical quantiles",
    y = "Standardized residuals",
    title = "Normal Q-Q",
    subtitle = deparse(modelA$call)
  )
```

Here we plot a normal Q-Q plot for the model residuals. Ideally, we would like to see all points lying perfectly along a line going through the origin.
In this case, we have some deviation from this straight line at the tails. This indicates a somewhat heavy left tail and light right tail, compared to the normal distribution.

Finally,

```{r}
# normality test
library(nortest) 
ad.test(rstudent(modelA))
```

Here, the "Anderson-Darling normality test" is performed on the *studentized residuals*. Due to a large value for A, we must reject the null-hypothesis of residual normality. The conclusions based on the plots above are therefore apparently wrong according to this test.

**Q5:** 

We now fit a new model, `modelB`, identical to `modelA` except for not using a *logarithmic* transformation for `FEV`.

```{r,eval=TRUE}
modelB <- lm(FEV ~ Age + Htcm + Gender + Smoke, data=lungcap)
summary(modelB)
```

Since we have the exact same covariates, we can assess the two models by comparing their $R^2$ statistics. Since $R_B^2 < R_A^2$, `modelA` is to be preferred if explanatory power is important.

Arguably, we should still choose `modelB` if it satisfies the error term normality condition, since this is an important assumption of the linear regression model.

```{r}
ad.test(rstudent(modelB))
```

We still can't conclude that error normality is definitely satisfied here, so we still choose `modelA` over `modelB`.

**Q6:** 

We now want perform the following hypothesis test

\begin{gather*}
\mathrm{H_0}: \beta_{\mathrm{age}} = 0 \\
\text{ vs. } \\
\mathrm{H_1}: \beta_{\mathrm{age}} \neq 0.
\end{gather*}

In order to test this, define the following $t$-distributed test statistic

$$
T_{\mathrm{age}} = \frac{\hat{\beta}_{\mathrm{age}} - \beta_{\mathrm{age}}}{\sqrt{\hat{\mathrm{Var}}(\hat{\beta}_{\mathrm{age}})}} \sim t_{n - 2}.
$$
The $p$-value for the test is defined by

$$
p\text{-value} := P(T_{\mathrm{age}} > t_{\mathrm{age}})
$$
We now calculate this value

```{r,eval=TRUE}
n <- nobs(modelA)
betaAge <- beta["Age"]
betaSE <- modelAEstimates$`Std. Error`[2]
tTestStatistic <- betaAge / betaSE
pValue <- 2 * pt(q = tTestStatistic, df = n - 2, lower.tail = FALSE)
pValue
```

The $p$-value is $\approx 7.1 \cdot 10^{-12}$, which coincides with the model summary output from earlier.

With a significance level of 5%, the critical value (which gives the cut-off) can be compared
with our $t$-statistic.

```{r}
alpha <- 0.05
criticalValue <- qt(p = alpha / 2, df = n - 2, lower.tail = FALSE)
kable(tibble("t-statistic" = tTestStatistic, "critical value" = criticalValue))
```

We can reject the null-hypothesis and conclude that $\beta_{\mathrm{age}} \neq 0$.

**Q7:** 

A $95\%$ confidence interval for $\beta_{\mathrm{age}}$ can be constructed with the same test statistic as above in the following way

\begin{align*}
P(-t_{\alpha / 2, n - 2} < T_{\mathrm{age}} < t_{\alpha / 2, n - 2}) &= 1 - \alpha \\
\implies
P(-t_{\alpha / 2, n - 2} < \frac{\hat{\beta}_{\mathrm{age}} - \beta_{\mathrm{age}}}{\hat{\mathrm{SE}}(\hat{\beta}_{\mathrm{age}})} < t_{\alpha / 2, n - 2}) &= 1 - \alpha \\
\implies
P(\hat{\beta}_{\mathrm{age}} - t_{\alpha / 2, n - 2} \cdot \hat{\mathrm{SE}}(\hat{\beta}_{\mathrm{age}}) < \beta_{\mathrm{age}} < \hat{\beta}_{\mathrm{age}} + t_{\alpha / 2, n - 2} \cdot \hat{\mathrm{SE}}(\hat{\beta}_{\mathrm{age}})) &= 1 - \alpha
\end{align*}

So we must find the interval $\hat{\beta}_{\mathrm{age}} \pm t_{\alpha / 2, n - 2} \cdot \hat{\mathrm{SE}}(\hat{\beta}_{\mathrm{age}})$.

```{r,eval=TRUE}
alpha <- 0.05
radius <- qt(p = alpha / 2, df = n - 2, lower.tail = FALSE) * betaSE
confidenceInterval <- tibble(lower = betaAge - radius, upper = betaAge + radius)
confidenceInterval %>% kable()
```

Notice that the 95% confidence interval for $\beta_{\mathrm{age}}$ is entirely positive.
We can therefore relatively confidently conclude that `age` has a positive effect on `FEV`, as the multiplicative effect becomes greater than one, which is really useful.
This positive 95% interval (i.e. not containing $0$) is also closely related to the fact that we rejected the null-hypothesis at a 5% $p$-value cut-off in the previous task,
since we can say with 95% confidence that $\beta_{\mathrm{age}}$ is nonzero.

**Q8:**

We now want to make a model prediction for a 16 year old non-smoking male of height 170 centimeters, $x_0$.

First we want to predict a value for `log(FEV)` by calculating

$$
\hat{\log({\mathrm{FEV}})} = x_0^T \hat{\beta}
$$
This can be done with the R-function `predict()` like this 

```{r,eval=TRUE}
new = data.frame(Age=16, Htcm=170, Gender="M", Smoke=0)
logPrediction <- as_tibble(
  predict(
    modelA,
    newdata = new,
    interval = "predict",
    type = "response",
    level = 0.95
  )
) %>%
  rename(
    lower = lwr,
    upper = upr
  )
print(logPrediction$fit)
```

Our best guess, based on `modelA`, for `log(FEV)` is thus $\approx 1.33$.

We can also construct a confidence interval for this predicted logarithmic value by using the following formula (given without proof)

$$
[{\bf x}_0^T \hat{\beta}-t_{\alpha/2,n-p-1}\hat{\sigma}\sqrt{1+{\bf x}_0^T({\bf X}^T{\bf X})^{-1}{\bf x}_0},~{\bf x}_0^T \hat{\beta}+t_{\alpha/2,n-p-1}\hat{\sigma}\sqrt{1+{\bf x}_0^T({\bf X}^T{\bf X})^{-1}{\bf x}_0}],
$$.

and exponentially transform the interval in order to get a 95% prediction interval for `FEV`.
We can use `predict()` for this purpose as well

```{r}
logInterval <- logPrediction[c("lower", "upper")]
FEVInterval <- exp(logInterval)
FEVInterval %>% kable()
```

Our 95% prediction interval is $[2.82, 5.01]$, which is a quite wide interval.
If we compare this interval to our `FEV` observations

```{r}
summary(lungcap$FEV)
```

the most useful thing we can conclude is that the individual likely has an above average `FEV`.

# Problem 2: Classification 

```{r}
library(class)# for function knn
library(caret)# for confusion matrices

raw = read.csv("https://www.math.ntnu.no/emner/TMA4268/2019v/data/tennis.csv")
M = na.omit(
  data.frame(
    y = as.factor(raw$Result),
    x1 = raw$ACE.1 - raw$UFE.1 - raw$DBF.1,
    x2 = raw$ACE.2 - raw$UFE.2 - raw$DBF.2
  )
)
set.seed(4268) # for reproducibility
tr <- sample.int(nrow(M), nrow(M) / 2)
trte <- rep(1, nrow(M))
trte[tr] <- 0
Mdf <- data.frame(M, "istest" = as.factor(trte))
```

**Q9:** 

Define $\mathcal{N}_x$ to be the set of the $K$ closest points to the point $x = (x_1, x_2)$.
The KNN istimator $\hat{y}(x) \in \{0, 1\}$ is determined by taking a "majority vote" of
its $N$ closest neighbors.
The KNN estimate of the posterior class probability becomes

\begin{gather*}
\hat{P}(Y = y | X = x)
=
\frac{1}{K} \sum_{i \in \mathcal{N_x}} I(y_i = y)
=
\begin{cases}
  \frac{1}{K} \sum_{i \in \mathcal{N_x}} y_i,~~~~~~~~\text{if } y = 1 \\
  1 - \frac{1}{K} \sum_{i \in \mathcal{N_x}} y_i,~\text{if } y = 0
\end{cases}
\end{gather*}

And the estimator can bayes classifier can be used to construct the estimator $\hat{y}(x)$

\begin{gather*}
\hat{y}(x) = 
\begin{cases} 
  1, ~~~~~\text{if } \hat{P}(Y = 1 | X = x) \geq 0.5 \\
  0, ~~~~~\text{otherwise}
\end{cases}
\end{gather*}

**Q10:** 

We now calculate the error rates of the training and test sets for $K = 1, 2, ..., 30$ and plot these results.

```{r,eval=TRUE}
dataset <- as_tibble(Mdf)
train <- dataset %>% filter(istest == "0") %>% select(x1, x2)
test <- dataset %>% filter(istest == "1") %>% select(x1, x2)

trainAnswers <- dataset %>% filter(istest == "0") %>% pull(y)
testAnswers <- dataset %>% filter(istest == "1") %>% pull(y)

train.e <- vector()
test.e <- vector()

knnErrorRates <- function(train, test, trainAnswers, testAnswers) {
  errorRates <- vector()
  for (k in 1:30) {
    testPredictions <- class::knn(
      train = train,
      test = test,
      cl = trainAnswers,
      k = k
    )
    errorRates <- append(
      errorRates,
      mean(testPredictions != testAnswers)
    )
  }
  return(errorRates)
}
test.e <- knnErrorRates(
  train = train,
  test = test,
  trainAnswers = trainAnswers,
  testAnswers = testAnswers
)
train.e <- knnErrorRates(
  train = train,
  test = train,
  trainAnswers = trainAnswers,
  testAnswers = trainAnswers
)
result <- tibble(
  K = 1:30,
  testError = test.e,
  trainError = train.e
)

result %>% ggplot(aes(x = K)) +
  geom_line(aes(y = testError, col = "Test set")) +
  geom_line(aes(y = trainError, col = "Training set")) +
  ylab("Error rate")
```

As expected, the training set has the lowest error rate for $K = 1$. The reason for the error not being
exactly zero at $K = 1$ is due to duplicate $x$ entries.

```{r, eval=TRUE}
set.seed(0)
ks <- 1:30 # Choose K from 1 to 30.
idx <- createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop. 
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv", 
# where folds are rows and values of $K$ are columns.
cv = sapply(
  ks,
  function(k){ 
    sapply(
      seq_along(idx),
      function(j) {
        yhat <- class::knn(
          train = M[tr[ -idx[[j]] ], -1],
          cl = M[tr[ -idx[[j]] ], 1],
          test = M[tr[ idx[[j]] ], -1],
          k = k
        )
        mean(M[tr[ idx[[j]] ], 1] != yhat)
      }
    )
  }
)
```


**Q11:** 

```{r, eval=TRUE}
cv.e <- colMeans(cv)
cv.se <- apply(cv, 2, sd) / sqrt(5)
k.min <- which.min(cv.e)
```

**Q12:** 

We now plot the misclassification errors.

```{r,eval=FALSE}
library(colorspace)
co <- rainbow_hcl(3)
par(mar = c(4,4,1,1) + 0.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type = "o", pch = 16, ylim = c(0, 0.7), col = co[2],
     xlab = "Number of neighbors", ylab = "Misclassification error")
arrows(ks, cv.e-cv.se, ks, cv.e+cv.se, angle = 90, length = .03,
       code = 3, col = co[2])
lines(ks, train.e, type = "o", pch = 16, ylim = c(0.5, 0.7), col = co[3])
lines(ks, test.e, type = "o", pch = 16, ylim = c(0.5, 0.7), col = co[1])
legend("topright", legend = c("Test", "5-fold CV", "Training"), lty = 1,
       col = co)
```

The bias of $\hat{y}(x)$ will increase with increasing $K$, but the variance will *decrease* with increasing $K$.
The variance is high for $K = 1$ because only the nearest neighbours is used and noise will therefore affect the model quite a lot.
The bias is low for $K = 1$ because the model will be very close to the training data.

**Q13:** 

A different strategy will now be used to choose $K$.

```{r,eval = TRUE}
k <- tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
size <- 100
xnew <- apply(M[tr,-1], 2, function(X) seq(min(X), max(X), length.out = size))
grid <- expand.grid(xnew[,1], xnew[,2])
grid.yhat <- knn(M[tr,-1], M[tr,1], k=k, test=grid)
np <- 300
par(mar = rep(2,4), mgp = c(1, 1, 0))
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels = .5, 
        xlab = expression("x"[1]), ylab = expression("x"[2]), axes = FALSE,
        main = paste0(k,"-nearest neighbors"), cex = 1.2, labels = "")
points(grid, pch = ".", cex = 1, col = grid.yhat)
points(M[1:np,-1], col = factor(M[1:np,1]), pch = 1, lwd = 1.5)
legend("topleft", c("Player 1 wins", "Player 2 wins"), 
       col = c("red", "black"), pch = 1)
box()
```

The line of interest is

```{r,eval=FALSE}
k <- tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
```

Instead of choosing the $K$ that results in the smallest CV error rate, $\hat{\mathrm{K}} := \mathrm{argmin}~\mathrm{CV}(\mathrm{K})$,
we instead choose the greatest $K$ which is within one standard deviation of $\hat{\mathrm{K}}$. More exactly,

$$
\mathrm{K} = \underset{\mathrm{K^*}}{\mathrm{max}} ~ \{K^* ~ | ~ \mathrm{CV}(\mathrm{K^*}) \leq \mathrm{CV}(\hat{\mathrm{K}}) + \mathrm{SE}(\hat{\mathrm{K}})\}
$$

This way, we are able to simplify our model within acceptable bounds from the minimal error.

**Q14:**

We now plot the receiver operating characteristics (ROC) curve.

```{r,eval = TRUE}
K <- tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)

KNNclass <- class::knn(train = M[tr,-1], cl = M[tr,1], test = M[-tr,-1], k = K,prob=TRUE)
KNNprobwinning <- attributes(KNNclass)$prob
KNNprob <- ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)

library(pROC)
player1ROC <- roc(response = M[-tr, 1], predictor = KNNprob, legacy.axes = TRUE)
ROC <- tibble(
  sensitivity = rev(player1ROC$sensitivities),
  specificity = rev(player1ROC$specificities)
)

# For stepribbon functionality
library(ggalt)
ROC %>% ggplot() +
  geom_ribbon(
    aes(
      x = specificity,
      ymin = 0,
      ymax = sensitivity,
      col = "AUC"
    ),
    fill = "red",
    alpha = 0.2,
    stat = "stepribbon"
  ) +
  geom_line(
    data = tibble(x = c(1, 0), y = c(0, 1)),
    aes(x = x, y = y, col = "Random guess"),
    linetype = "dashed"
  ) +
  geom_step(
    aes(x = specificity, y = sensitivity, col = "ROC")
  ) +
  labs(
    title = "ROC curve",
    caption = paste(
      c("AUC: ", signif(player1ROC$auc, 3)),
      collapse = " "
    )
  ) +
  scale_x_reverse() +
  coord_equal() +
  ylab("sensitivity")
```

This curve shows the relationship between the *specificity* and *sensitivity* of the model as the threshold probability goes from $0$ to $1$,
defined as

\begin{gather*}
\text{sensitivity} := \frac{\text{Number of true positives}}{\text{Number of positives}} = \frac{\mathrm{TP}}{\mathrm{P}} \\
\text{specificity} := \frac{\text{Number of true negatives}}{\text{Number of negatives}} = \frac{\mathrm{TN}}{\mathrm{N}}
\end{gather*}

Thus, these extreme thresholds (probability cut-off) must therefore imply the following

\begin{gather*}
\text{threshold} \gets 0 \implies 
\begin{cases}
  \text{sensitivity} = 1 \\
  \text{sensitivity} = 0
\end{cases}
\\
\text{threshold} \gets 1 \implies 
\begin{cases}
  \text{sensitivity} = 0 \\
  \text{sensitivity} = 1
\end{cases}
\end{gather*}

The $\mathrm{AUC} \in [0, 1]$ is the area under the $\mathrm{ROC}$ curve. The best case scenario is when the $\mathrm{ROC}$ curve "hugs" the
upper left corner, resulting in $\mathrm{AUC} = 1$. In our case, we have $\mathrm{AUC} \approx 0.818$.

Random guessing can be thought of as drawing samples $u \sim \mathcal{U}(0, 1)$ and comparing it to the threshold probability in order to make
a prediction. Since $P(u < p) = p$, the resulting specificity-sensitivity graph becomes a straight line (on average), and the $\mathrm{AUC}$ becomes $0.5$.

**Q15:**

```{r,eval=TRUE}
tb <- tibble(
  x1 = M$x1,
  x2 = M$x2,
  y = M$y,
  yHat = as.integer(M$x1 > M$x2),
  istest = as.factor(trte)
)
p1WinsPoly <- tibble(
  x1 = c(-100, 20, 20),
  x2 = c(-100, -100, 20)
)
p2WinsPoly <- tibble(
  x1 = c(-100, 20, -100),
  x2 = c(-100, 20, 20)
)
tb %>% ggplot() +
  aes(x = x1, y = x2) +
  geom_polygon(
    data = p1WinsPoly,
    aes(x = x1, y = x2),
    alpha = 0.2,
    fill = "red"
  ) +
  geom_polygon(
    data = p2WinsPoly,
    aes(x = x1, y = x2),
    alpha = 0.2,
    fill = "black"
  ) +
  geom_point(
    aes(col = y, x = x1, y = x2, shape = istest),
    size = 1
  ) +
  geom_abline(
    aes(
      slope = 1,
      intercept = 0,
      linetype = "Decision boundary"
    )
  ) +
  xlab(expression("x"[1])) +
  ylab(expression("x"[2])) +
  scale_color_manual(
    name = "Ground truth",
    labels = c("Player 2 wins", "Player 1 wins", ""),
    values = c("black", "red", "gray")
  ) +
  scale_linetype_manual(
    name = "Argmax estimator",
    labels = "Decision Boundary",
    values = "dashed"
  ) +
  scale_shape_manual(values = c(1, 2)) +
  scale_x_continuous(limits = c(-100, 20), expand = c(0, 0)) +
  scale_y_continuous(limits = c(-100, 20), expand = c(0, 0)) +
  theme(
    axis.ticks = element_blank(),
    axis.text = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank()
  )
```

```{r}
train <- tb %>% slice(-tr)
argmaxConfMatrix <- confusionMatrix(table(train$yHat, train$y))
knnConfMatrix <- confusionMatrix(table(KNNclass, train$y))
print(argmaxConfMatrix)
print(knnConfMatrix)
```

These two predictors perform really similarly, since the 95% confidence interval for the accuracy is quite overlapping.
The KNN predictor has better specificity, while the argmax predictor has better sensitivity. 
We prefer the argmax predictor since it has better overall accuracy.

# Problem 3: Bias-variance trade-off 

Here you see how to write formulas with latex (needed below)

$$
\hat{\boldsymbol \beta}=({\bf X}^T{\bf X})^{-1}{\bf X}^T{\bf Y}
$$

**Q16:** 

First, let's find the expected value for $\hat{beta}$. Here we will use that a linear combination, $C$, of a normal vector, $Y$, satisfies $E[CY] = C E[Y]$, and that $E[\epsilon] = 0$:

\begin{align*}
E[\hat{\beta}]
&= E\left[(X^T X)^{-1} X^T Y\right]
= (X^T X)^{-1} X^T E[Y] \\
&= (X^T X)^{-1} X^T E[X \beta + \epsilon]
= (X^T X)^{-1} X^T X \beta = \beta
\end{align*}

This is an unbiased estimator! Now onto the variance-covariance matrix for $\hat{\beta}$.
Here, we will use the fact that $\mathrm{Cov}(CY) = C \mathrm{Cov}(Y) C^T$, and $\mathrm{Cov}(Y) = \sigma^2 I$.

\begin{align*}
\mathrm{Cov}(\hat{\beta})
&= \mathrm{Cov}\left((X^T X)^{-1} X^T Y\right)
= (X^T X)^{-1} X^T \mathrm{Cov}(Y) ((X^T X)^{-1} X^T)^T \\
&= (X^T X)^{-1} X^T \sigma^2 I ((X^T X)^{-1} X^T)^T
= (X^T X)^{-1} \sigma^2
\end{align*}

So the result is that $\hat{\beta} \sim N_{p + 1}(\beta, \sigma^2 (X^T X)^{-1})$.

**Q17:** 

Now, we find the expected value of $\hat{f}(x_0)$.

$$
E\left[\hat{f}(x_0)\right]
= E\left[x_0^T \hat{\beta}\right]
= x_0^T E[\hat\beta]
= x_0^T \beta
= E[Y_0]
$$

So this is an unbiased predictor! Now, the variance of the predictor.

$$
\mathrm{Var}(\hat{f}(x_0))
= \mathrm{Var}(x_0^T \hat{\beta})
= x_0^T \mathrm{Cov}(\hat \beta) x_0
= \sigma^2 x_0^T (X^T X)^{-1} x_0
$$

So, we have $\hat{f}(x_0) \sim N_1(x_0^T \beta,~\sigma^2 x_0^T (X^T X)^{-1} x_0)$.

**Q18:** 

We will use that for a random variable, $Z$, we have $\mathrm{E}\left[Z^2\right] = \mathrm{Var}(Z) + \mathrm{E}[Z]^2$.
Also, since $Y_0$ and $\hat{f}(x_0)$ are independent, then $\mathrm{E}[Y_0 \hat{f}(x_0)] = \mathrm{E}[Y_0] \mathrm{E}[\hat{f}(x_0)]$.

\begin{gather*}
\text{E}[(Y_0-\hat{f}({\bf x}_0))^2] \\
= \mathrm{E}\left[ Y_0^2 + \hat{f}(x_0)^2 - 2 Y_0 \hat{f}(x_0) \right] \\
= 
  \mathrm{E}\left[ Y_0^2 \right]
  + \mathrm{E}\left[ \hat{f}(x_0)^2 \right]
  - \mathrm{E}\left[ 2 Y_0 \hat{f}(x_0) \right] \\
= 
  \mathrm{Var}(Y_0) + \mathrm{E}\left[ Y_0 \right]^2
  + \mathrm{Var}\left(\hat{f}(x_0)\right) + \mathrm{E}\left[ \hat{f}(x_0) \right]^2
  - 2 \cdot \mathrm{E}\left[ Y_0 \right] \mathrm{E} \left[ \hat{f}(x_0) \right] \\
= 
  \mathrm{Var}(Y_0) + f(x_0)^2
  + \mathrm{Var}\left(\hat{f}(x_0)\right) + \mathrm{E}\left[ \hat{f}(x_0) \right]^2
  - 2 \cdot f(x_0) \mathrm{E} \left[ \hat{f}(x_0) \right] \\
= 
  \mathrm{Var}(Y_0)
  + \mathrm{Var}\left(\hat{f}(x_0)\right)
  + \left( \mathrm{E}\left[\hat{f}(x_0)\right] - f(x_0) \right)^2 \\
=
  \sigma^2 I
  + \sigma^2 x_0^T (X^T X)^{-1} x_0
  + (x_0^T \beta - x_0^T \beta)^2 \\
=
  \sigma^2 I
  + \sigma^2 x_0^T (X^T X)^{-1} x_0
  + 0^2 \\
= \text{Irreducible error} + \text{Variance} + \text{Bias}^2
\end{gather*}


**Q19:** 

We now look at the ridge estimator, $\widetilde{\beta}$,

$$
\widetilde{\beta} := (X^T X+\lambda I)^{-1} X^T Y,~~~\lambda \in [0, \infty]
$$

First, we calculate the expected value.

$$
\mathrm{E}\left[ \widetilde{\beta} \right]
= \mathrm{E}\left[(X^T X+\lambda I)^{-1} X^T Y \right]
= (X^T X+\lambda I)^{-1} X^T \mathrm{E}\left[ Y \right]
= (X^T X+\lambda I)^{-1} (X^T X) \beta
$$

Since $\widetilde \beta \neq \hat \beta$, this is a biased estimator!
The variance becomes

\begin{gather*}
\mathrm{Var}(\widetilde{\beta})
= \mathrm{Var}\left((X^T X+\lambda I)^{-1} X^T Y \right) \\
= (X^T X+\lambda I)^{-1} X^T~\mathrm{Var}(Y)~ \left((X^T X+\lambda I)^{-1} X^T \right)^T \\
= \sigma^2 (X^T X+\lambda I)^{-1} (X^T X) (X^T X+\lambda I)^{-1}
\end{gather*}

So we end up with $\widetilde{\beta} \sim N_{p+1}\left((X^T X+\lambda I)^{-1} (X^T X) \beta,~\sigma^2 (X^T X+\lambda I)^{-1} (X^T X) (X^T X+\lambda I)^{-1}\right)$.

**Q20:** 

Now we find the expected value of the estimator $\widetilde{f}(x_0) = x_0^T \widetilde{\beta}$.

$$
\mathrm{E}\left[\widetilde{f}(x_0) \right]
= \mathrm{E}\left[x_0^T \widetilde{\beta} \right]
= x_0^T \mathrm{E}\left[\widetilde{\beta} \right]
= x_0^T (X^T X+\lambda I)^{-1} (X^T X) \beta
$$

And the variance.

$$
\mathrm{Var}\left(\widetilde{f}(x_0) \right)
= \mathrm{Var}\left(x_0^T \widetilde{\beta} \right)
= x_0^T \mathrm{Cov}\left( \widetilde{\beta} \right) x_0
= \sigma^2 x_0^T (X^T X+\lambda I)^{-1} (X^T X) (X^T X+\lambda I)^{-1} x_0
$$

And therefore we have $\widetilde{f}(x_0) \sim N_1\left(x_0^T (X^T X+\lambda I)^{-1} (X^T X) \beta,~\sigma^2 x_0^T (X^T X+\lambda I)^{-1} (X^T X) (X^T X+\lambda I)^{-1} x_0 \right)$.

**Q21:** 

Again, we decompose the MSE, skipping the initial decomposition proof this time

\begin{gather*}
\text{E}[(Y_0-\widetilde{f}({\bf x}_0))^2] \\
=
  \left(\text{E}\left[ \widetilde{f}(x_0) \right] - f(x_0) \right)^2
  + \text{Var}\left( \widetilde{f}({\bf x}_0) \right)
  + \text{Var}(\varepsilon) \\
=
  \left(x_0^T (X^T X+\lambda I)^{-1} (X^T X) \beta - x_0^T \beta \right)^2
  + \sigma^2 x_0^T (X^T X+\lambda I)^{-1} (X^T X) (X^T X+\lambda I)^{-1} x_0
  + \sigma^2 I \\
=  \text{Bias}^2 + \text{Variance} + \text{Irreducible error}
\end{gather*}

**Q22:** 

```{r}
values <- dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/BVtradeoffvalues.dd")
X <- values$X
dim(X)
x0 <- values$x0
dim(x0)
beta <- values$beta
dim(beta)
sigma <- values$sigma
sigma
```

Hint: we perform matrix multiplication using `%*%`, transpose of a matrix `A` with `t(A)` and inverse with `solve(A)`. 

```{r,eval=FALSE}
sqbias <- function(lambda, X, x0, beta) {
  p <- dim(X)[2]
  value <- #HERE YOU FILL IN
  return(value)
}
thislambda <- seq(0,2,length=500)
sqbiaslambda <- rep(NA,length(thislambda))
for (i in 1:length(thislambda)) {
  sqbiaslambda[i] <- sqbias(thislambda[i],X,x0,beta)
}
plot(thislambda, sqbiaslambda, col = 2, type = "l")
```

**Q23:** 

```{r,eval=FALSE}
variance <- function(lambda, X, x0, sigma) {
  p <- dim(X)[2]
  inv <- solve(t(X)%*%X+lambda*diag(p))
  value <- #HERE YOU FILL IN
  return(value)
}
thislambda <- seq(0,2,length=500)
variancelambda <- rep(NA,length(thislambda))
for (i in 1:length(thislambda)) {
  variancelambda[i] <- variance(thislambda[i], X, x0, sigma)
}
plot(thislambda, variancelambda, col = 4, type = "l")
```


**Q24:** 

```{r,eval=FALSE}
tot <- sqbiaslambda+variancelambda+sigma^2
which.min(tot)
thislambda[which.min(tot)]
plot(thislambda, tot, col = 1, type = "l", ylim = c(0,max(tot)))
lines(thislambda, sqbiaslambda, col = 2)
lines(thislambda, variancelambda, col = 4)
lines(thislambda, rep(sigma^2,500), col = "orange")
abline(v = thislambda[which.min(tot)], col = 3)
```
