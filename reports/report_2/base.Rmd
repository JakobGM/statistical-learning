---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 2: Group XYZ"
author: "Martinussen, Jakob Gerhard & Wilson, Alm"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    latex_engine: xelatex
    keep_tex: true
header-includes:
  - \usepackage{fontspec}
  - \setmonofont{DejaVu Sans Code}
  - \makeatletter
  - \def\verbatim@nolig@list{}
  - \makeatother
  - \newcommand{\mat}[1]{\boldsymbol{#1}}

  - \usepackage{bm}
  - \usepackage[a]{esvect}
  - \renewcommand{\vec}[1]{\vv{\bm{#1}}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(results = "hold")
```

# Problem 1: Regression [6 points]

We start by importing the data set used in this problem.

```{r}
all <- dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/diamond.dd")
dtrain <- all$dtrain
dtest <- all$dtest
```

**Q1**: Would you choose `price` or `logprice` as response variable? Justify your choice. Next, plot your choice of response pairwise with `carat`, `logcarat`, `color`, `clarity` and `cut`. Comment.

We choose `logprice` as the response variable, since we postulate that diamond qualities have a *multiplicative* effect on the price, not *additive*. A logarithmic response will result in such an interpretation of the regression coefficients. We now create the plots as instructed.

```{r}
library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)

# Combine both training and testing set for plotting
dall <- bind_rows(dtrain, dtest)

# Plot categorical covariates
dall %>%
  gather(key, value, c("cut", "color", "clarity")) %>%
  ggplot(aes(y = logprice)) +
  geom_point(aes(x = value), alpha = 0.3) +
  facet_grid(.~key, scales = "free")
```

```{r}
# Plot continuous covariates
dall %>%
  gather(key, value, c("carat", "logcarat")) %>%
  ggplot(aes(y = logprice)) +
  geom_point(aes(x = value)) +
  facet_grid(.~key, scales = "free")
```

We observe that `logcarat` *seems* to be the covariate that has the most linear correlation with `logprice`, while `carat` does not.
It is difficult to see any clear trends in the plots of the categorical covariates `clarity`, `color`, and `cut`.

**Q2:** What is the predicted price of a diamond weighting 1 carat. Use the closest 20% of the observations. 

We now use the local regression model $\texttt{logprice} = \beta_0 + \beta_1 \texttt{carat}+ \beta_2 \texttt{carat}^2$ weighted by the tricube kernel $K_{i0}$.

```{r}
model1 <- loess(logprice ~ carat + carat^2, data = dtrain, span = 0.2)
```

We make a price prediction for a 1 carat diamond, using the closest 20\% of the observations:

```{r}
new_observation <- list(carat = 1)
price_prediction <- 10 ^ predict(model1, new_observation)
sprintf("$%.2f USD", price_prediction)
```

So the prediction is that a 1 carat diamond will cost $\simeq$ $5100 USD.

**Q3:** What choice of $\beta_1$, $\beta_2$ and $K_{i0}$ would result in KNN-regression?

Let $\mathcal{N}_0$ be the index set for the $K$ nearest (euclidean) neighbours of the predictor variable $\vec{x}_0$.
Now assign the following values

\begin{align*}
  \beta_1 &\leftarrow 0 \\
  %
  \beta_2 &\leftarrow 0 \\
  %
  K_{i0} &\leftarrow I(i \in \mathcal{N}_0).
\end{align*}

This yields the following expression to be minimized by the local regression

\begin{equation*}
  \sum_{i = 1}^n K_{i0}(y_i - \beta_0 - \beta_1 x_i - \beta_2x_i^2)^2
  =
  \sum_{i \in \mathcal{N}_0} (y_i - \beta_0)^2,
\end{equation*}

The minimizer for this expression is $\beta_0 = \frac{1}{K} \sum_{i \in \mathcal{N}_0} y_i = \bar{y}_{\mathcal{N}_0}$.
Here we have denoted $\bar{y}_{\mathcal{N}_0}$ as the mean of the responses of the $K$ nearest neighbours to $\vec{x}_0$.
This results in the following predictor

$$
  \hat{f}(\vec{x}_0) = \bar{y}_{\mathcal{N}_0},
$$

which is in fact the predictor of a KNN-regression.

**Q4:** Describe how you can perform model selection in regression with AIC as criterion.

The Akaike Information Criterion (AIC) is defined as

$$
\text{AIC} := -2 \cdot l(\hat{\beta}_M, \tilde{\sigma}^2) +2(\lvert M\rvert +1),
$$

where $\hat{\beta}_M$ is the estimated coefficient vector for a model with $M$ regression parameters, and $l(\cdot)$ is the logarithm of the maximum likelihood estimator for that model.
How you calculate the maximum likelihood depends on what type of regression you perform.
We use $|M| + 1$, because we have to add the estimated variance of the error terms $\epsilon_i$.

When we do model selection, i.e. picking between different candidate models, we can pick the model with the *lowest* value for AIC.

**Q5:** What are the main differences between using AIC for model selection and using cross-validation (with mean squared test error MSE)?

Here are the main differences:

* CV requires you to split your data set into *training* and *testing* subsets. This results in smaller sample sizes for fitting the models, and can result in an overestimation of the validation set error.
* CV test MSE can have high variability for smaller sample sizes, since it is dependent on which observations are included/excluded.

**Q6:** See the code below for performing model selection with `bestglm()` based on AIC. What kind of contrast is used to represent `cut`, `color` and `clarity`? Write down the final best model and explain what you can interpret from the model.

```{r}
library(bestglm)
ds <- as.data.frame(
  within(
    dtrain,
    {
      y = logprice    # setting response
      logprice = NULL # not include as covariate
      price = NULL    # not include as covariate
      carat = NULL    # not include as covariate
    }
  )
)

fit <- bestglm(Xy=ds, IC="AIC")$BestModel
summary(fit)
```

From the summary we can see that `cut`, `color`, and `clarity` are *dummy coded* with respectively `cutFair`, `colorD`, and `clarityI1` as reference categories.

The final best model is `logprice ~ 1 + logcarat cut + color + clarity + xx`, or in mathematical form

$$
\texttt{logprice}_i
= \\
  \beta_0 \\
  + \beta_1 \cdot x_{i, \texttt{logcarat}} \\
  + \beta_2 \cdot x_{i, \texttt{cutGood}} + ... + \beta_5 \cdot x_{i, \texttt{cutIdeal}} \\
  + \beta_6 \cdot x_{i, \texttt{colorE}} + ... + \beta_{11} \cdot x_{i, \texttt{colorJ}} \\
  + \beta_{12} \cdot x_{i, \texttt{claritySI2}} + ... + \beta_{18} \cdot x_{i, \texttt{clarityIF}} \\
  + \beta_{19} \cdot x_{\texttt{xx}} \\
  + \epsilon_i.
$$

A `price` prediction for the covariate vector $\vec{x}_0$ is constructed in the following manner

$$
\widehat{\texttt{logprice}}
=
\vec{\hat{\beta}}^T \vec{x}_0 \\
\implies
\widehat{\texttt{price}}
=
10^{\vec{\hat{\beta}}^T \vec{x}_0} = 10^{\beta_0} \cdot \prod_{i = 1}^{19} 10^{\beta_i \vec{x}_{0,i}}.
$$

The tenth power of the regression coefficients are therefore of interest.

```{r}
library(kableExtra)
beta_hat <- coefficients(fit)
10^beta_hat %>% kable(col.names = "$10^{\\hat{\\beta}_i}$")
```

Several things can be interpreted from this. The `(Intercept)` represents a 1 carat diamond, i.e. 0 `logcarat`, with a *fair* cut, *D* color category, and *I1* clarity category. Such a diamond is predicted to cost $\simeq$ \$968 USD.
All other covariates must be interpreted *relative to* this "reference diamond". For instance, since $10^{\vec{\hat{\beta}}_{\texttt{cutIdeal}}} \approx 1.135$, we would expect a diamond with an *ideal cut* to cost 13.5\% more than the "reference diamond", all other properties being equal.

**Q7:** Calculate and report the MSE of the test set using the best model (on the scale of `logprice`).

```{r}
best_subset_test_mse <- mean((dtest$logprice - predict.lm(fit, dtest))^2)
print(best_subset_test_mse)
```

The test MSE is $\simeq 0.0036$.


**Q8:** Build a model matrix for the covariates `~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1`. What is the dimension of this matrix?

We build the design matrix by using the `model.matrix()` function.

```{r}
lasso_formula <- ~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1
model_matrix <- model.matrix(
  lasso_formula,
  data = dtrain
)
```

Since we use dummy coding of categorical covariates, each covariate with $c$ categories result in $c - 1$ columns in the design matrix.
But in this case, we have no intercept, so one of the categorical covariates will have $c$ columns, one for each covariate.
We have 3 categorical variables, with 5, 8, and 7 categories. This results in 17 columns, instead of 16 if we had an intercept in our model.
In addition, we have 6 continuous covariates.
All together this results in 24 columns in the design matrix and $n$ rows, where $n$ equals the number of observations, in this case $5000$.
To confirm, we can check

```{r}
dim(model_matrix)
```

This coincides.


**Q9:** Fit a lasso regression to the diamond data with `logprice` as the response and the model matrix given in Q8. How did you find the value to be used for the regularization parameter?

We will fit a lasso regression model to the design matrix from **Q8**.
In order to choose a suitable value for the tuning parameter $\lambda$, we use 10-fold cross validation and the 1 standard deviation rule as explained in exercise 1. We let `glmnet` choose a suitable range of $\lambda$ values for us.


```{r}
library(glmnet)
cv.out <- cv.glmnet(
  x = model_matrix,
  y = dtrain$logprice,
  alpha=1,          # Lasso
  nfolds=10,        # 10-fold CV
  standardize=TRUE  # Standardize coefficients, see: https://think-lab.github.io/d/205/#5
) 
```

The cross validation MSE can be plotted against $\lambda$.

```{r}
plot(cv.out)
```

The two vertical lines correspond to the $\lambda$ with the least CV MSE (left) and the $\lambda$ chosen with the 1SD-rule (right), with the specific values

```{r}
print(cv.out$lambda.min)
print(cv.out$lambda.1se)
```

The chosen regularization parameter is therefore $\lambda \approx 5.2 \cdot 10^{-4}$, which is within one standard deviation of $\lambda_{\text{min}} \approx 1.2 \cdot 10^{-4}$.

**Q10:** Calculate and report the MSE of the test set (on the scale of `logprice`).

We can now calculate the test MSE.

```{r}
lasso_model <- cv.out$glmnet.fit
chosen_lambda <- cv.out$lambda.1se
xtest <- model.matrix(lasso_formula, data = dtest)
predictions <- predict(lasso_model, s = chosen_lambda, newx = xtest)
lasso_test_mse <- mean((dtest$logprice - predictions) ^ 2)
print(lasso_test_mse)
```

The test MSE is $\approx 3.88 \cdot 10^{-3}$ on the scale of `logprice`.


**Q11:** A regression tree to model is built using a _greedy_ approach. What does that mean? Explain the strategy used for constructing a regression tree.

A _greedy_ approach means that the apparently best choice is taken at any one time, not considering the possibility of a suboptimal choice may be taken in order to achieve an *overall* better result further down the road. Once a decision has been made, no backtracking is performed, and the choice is "locked in".

The idea of a regression tree is to take $n$ observation pairs $(\vec{x}_i, Y_i)$ and separating them into $J$ partitions of the predictor space, $R_1, ..., R_J$.
Predictions are made by identifying the region which a predictor falls within and then taking the average response of all the pairs of that region.

A prediction, $\hat{f}_{R_j}$, for $\vec{x}_0$ belonging to the region $R_j$, is therefore calculated as

$$
\hat{f}_{R_j} = \frac{1}{|R_j|} \sum_{i: x_i \in R_j} y_i,
$$

where $|R_j|$ is the number of observed pairs placed within region $R_j$.

Now a question remains; how to construct these regions? We use a greedy approach called *recursive binary splitting*. Define the parametrized regions

\begin{gather*}
  R_1(j, s) := \{\vec{x}~|~x_j < s\}, \\
  R_2(j, s) := \{\vec{x}~|~x_j \geq s\}.
\end{gather*}

We want to pick the predictor $j$ and split-off point $s$ that minimizes the sum of the residual sum of squares of both sub-partition, i.e.

$$
(j, s)
=
\underset{(j, s)}{\text{argmin}}
\left\{
\sum_{i: x_i \in R_1(j, s)} (y_i - \hat{f}_{R_1(j, s)})^2
+
\sum_{i: x_i \in R_2(j, s)} (y_i - \hat{f}_{R_2(j, s)})^2
\right\}
$$

We choose $(j, s)$ recursively without backtracking (thus greedy), until some stopping criterion is reached.
The stopping criterion can the number of members of the resulting region being small enough, or when the reduction in RSS is smaller than some threshold.

**Q12:** Is a regression tree a suitable method to handle both numerical and categorical covariates? Elaborate.

Yes, regression trees can handle mixed features, which is one of its advantages.
Instead of deciding node traversal by a "less than" operator, as in the numerical case, a decision can be made by a "containment" operator in the categorical case. For instance if $x_j$ is a categorical covariate representing if a patient is smoking (coded as $1$) or not (coded as $0$), a splitting criterion could be

\begin{gather*}
  R_1 = \{ \vec{x}~|~x_j = 1 \} \\
  R_2 = \{ \vec{x}~|~x_j = 0 \}.
\end{gather*}

**Q13:** Fit a (full) regression tree to the diamond data with `logprice` as the response (and the same covariates as for c and d), and plot the result. Comment briefly on you findings.

We now fit a full regression tree for the diamond data.

```{r}
library(tree)
diamond_formula <- logprice~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1
full_tree <- tree(
  diamond_formula,
  data = dtrain
)
```

The decision tree can be visualized together with the partition space.
We will transform the `logprice` to `price` for easier interpretability.

```{r}
# For deep copy of data-structures
library(rlang)

# Plot side-by-side
par(mfcol = c(1, 2))

# Convert fitted values from logprice to price
plot_tree <- duplicate(full_tree, shallow = FALSE)
plot_tree$frame$yval <- 10 ^ plot_tree$frame$yval

# Plot the decision tree together with decision criteria
plot(plot_tree, type = "proportional")
text(plot_tree, pretty = 1, cex = 0.5)

# Plot the partitioning of the xx-yy plane
partition.tree(plot_tree, cex = 0.4)
```

The length of the edges in the tree on the left hand side are proportional to the reduction in RSS.
We can see that the first split, $\texttt{yy} < 5.665$, by far reduces the RSS the most. The tree algorithm also decides that *above* this split, only $\texttt{yy}$ matters, and $\texttt{xx}$ is ignored. The greater $\texttt{xx}$ and $\texttt{yy}$ is, the greater the price prediction.
Lastly, it is interesting that solely $\texttt{xx}$ and $\texttt{yy}$ are chosen, all of the other covariates have been discarded. Using this model as a human is quite easy, as there are only two covariates to consider.


**Q14:** Calculate and report the MSE of the test set (on the scale of `logprice`).

We can now calculate the test MSE on the scale of `logprice`.

```{r}
tree_predictions <- predict(full_tree, dtest)
tree_test_mse <- mean((tree_predictions - dtest$logprice)^2)
print(tree_test_mse)
```

The test MSE is $\approx 1.72 \cdot 10^{-2}$, a whole order of magnitude greater than the lasso model fitted earlier. It should be noted, though, that this model is *substantially* less complex than the lasso model.


**Q15:** Explain the motivation behind bagging, and how bagging differs from random forest? What is the role of bootstrapping?

A problem with decision trees is that they often portray a large amount of variance, since small changes in the predictors will often result in very different trees. One solution to this problem is *bootstrap aggregation*, more commonly known as *bagging*.

The idea is to draw a sufficiently large amount, $B$, bootstrap replicates from the training set and fit a tree, $\hat{f}_{b}(\vec{x})$, for every single replicate, $X_b$.
The resulting predictor, $\hat{f}_{\text{avg}}(\vec{x})$ is the *average* of all those trees,

$$
\hat{f}_{\text{avg}}(\vec{x}) = \frac{1}{B} \sum_{b = 1}^{B} \hat{f}_b (\vec{x}).
$$

The motivation is this, assume that we have $B$ i.i.d. observations of the random variable $X$ with mean $\mu$ and variance $\sigma^2$. The unbiased mean estimator, $\bar{X} = \frac{1}{B} \sum_{b = 1}^B X_b$, has a variance of

$$
Var(\bar{X})
=
\mathrm{Var}\left( \frac{1}{B} \sum_{b = 1}^B X_b \right)
=
\frac{1}{B^2} \sum_{b = 1}^B \mathrm{Var}(X_b) = \frac{\sigma^2}{B}
$$

The variance of the estimator is thus inversely proportional to the number of independent samples $B$. This effect can be *simulated* by generating $B$ bootstrap data sets; they are not completely independent but is better than nothing.

Bagging is not the end all solution, however. If there is a really strong predictor in the training set, then this predictor will end up being used in the first split in all the bootstrap replicated trees.
This results in the trees becoming highly correlated, and we lose much of the variance reduction which we expect from the averaging of the trees. A *random forest* aims to solve this by randomly removing a fraction of the covariates in each split, instead of using *all* the covariates as we do in *bagging*. This is intended to decorrelates the trees.

**Q16:** What are the parameter(s) to be set in random forest, and what are the rules for setting these?

There are two parameters of interest when creating a *random forest*.
The first is the number of trees, $B$. This is *not* a tuning parameter, however, and the larger the better. Pick a value that is sufficiently large while being computationally feasible.

The other parameter is $m$, the number of covariates to be randomly kept in each split. First off, we require $m < p$, of course. For very correlated predictors, $m$ is chosen to be small.
A good *a priori* rule-of-thumb is to choose $m \approx \sqrt{p}$ for classification and $m = p / 3$ for regression. This has been derived from simulations.

**Q17:** Boosting is a popular method. What is the main difference between random forest and boosting?

A random forest is created by averaging $B$ trees fitted in parallel with the use of bootstrapping. The trees not "care about" each other.

Boosting is different in that it is an *iterative* or *sequential* method. It first builds a decision tree with $d$ splits, and calculates the residuals of this tree.
After this, a *new* tree is fitted on the residuals. The original tree is updated by adding a shrunken version of the new tree. This is repeated $B$ times, always fitting new trees on the new residuals.

**Q18:** Fit a random forest to the diamond data with `logprice` as the response (and the same covariates as before). Comment on your choice of parameter (as decribed in Q16).

We fit a random forest with the same formula.

```{r, include = FALSE}
library(randomForest)
```

```{r, cache = TRUE}
random_forest <- randomForest(
  diamond_formula,
  data = dtrain,
  mtry = dim(model_matrix)[2] / 3,
  importance = TRUE
)
```

We have here chosen `mtry = 24 / 3 = 8` since we have a continuous response and this is a regression tree. With other words, we are using the rule-of-thumb $m = p / 3$.

**Q19:** Make a variable importance plot and comment on the plot. Calculate and report the MSE of the test set (on the scale of `logprice`).

In order to interpret this *ensemble method* we can make a variable importance plot.

```{r}
varImpPlot(random_forest)
```

We can see that `yy` is decidedly best at decreasing the total node impurity, while `clarity` and `color` are best when using the alternative random out-of-bag importance method. There is no overlap between the 2 best covariates in each method.

We can now calculate the MSE for the test set on the scale of `logprice`.

```{r}
random_forest_predictions <- predict(random_forest, dtest)
random_forest_test_mse <- mean((random_forest_predictions - dtest$logprice)^2)
print(random_forest_test_mse)
```

We have a test MSE of $\approx 2.36 \cdot 10^{-3}$.


**Q20:** Finally, compare the results from c (subset selection), d (lasso), e (tree) and f (random forest): Which method has given the best performance on the test set and which method has given you the best insight into the relationship between the price and the covariates?

Let $\mat{X}$

We can now compare the test MSE scores of the four previous models.

```{r}
test_mse <- tibble(
  best_subset = best_subset_test_mse,
  lasso = lasso_test_mse,
  tree = tree_test_mse,
  random_forest = random_forest_test_mse
)
t(test_mse) %>% kable(col.names = c("Test MSE"))
```

We can see that the random forest has performed the best on the test set.
The random forest is difficult to interpret though, especially since the two importance plots do not coincide.
The tree model is much simpler to interpret, it condenses all the information down to just two covariates, `xx` and `yy`, making it easy to print out on a piece of paper and taking with you to the Jeweler.


# Problem 2: Unsupervised learning [3 points]

**Q21:** What is the definition of a principal component score, and how is the score related to the eigenvectors of the matrix \({\hat {\mat R}}\). 

Assume that you have an $n \times p$ design matrix $\mat{X}$, with a standardized version $\mat{Z}$ where the columns have been centered (having mean $0$) and scaled (having variance $1$).
We want to linearly transform the set of (possibly) correlated covariates into a new orthogonal basis where the *new* variables are linearly uncorrelated while retaining as much of the information (variance) in the data as possible.
These new variables, $\mat{W}_m$, are called *principal components* and the problem can be formally defined as trying to solve

\begin{align}
  \label{eq:decomposition}
  &\mat{W}_m
  =
  \hat{\mat{R}} \mat{\phi}_m
  =
  \sum_{j = 1}^p \phi_{jm} \hat{\mat R}_j,
    & \forall m: ||\mat{\phi}_{m}||_{2} &= 1 \nonumber \\
  %
  &\mathrm{Var}(\mat{W}_i) \geq \mathrm{Var}(\mat{W}_{j})
    &\forall i,j: i &< j \\
  %
  &\mat{W}_i \perp \mat{W}_j,
    & \forall i,j: i &\neq j \nonumber.
\end{align}

The new variables are linear combination of the original columns.
Since the components are sorted in decreasing order of variance contributed, we can subset the first $M$ components in order to reduce the dimensionality of the original data set while retaining the maximum amount of information in the data set.

We will now explain how to solve this problem.
The estimated covariance matrix, $\hat{\mat{R}} \in \mathbb{R}^{p \times p}$, for the standardized design matrix, $\mat{Z}$, is given as

$$
{\hat {\mat{R}}}
=
\frac{1}{n-1} \sum_{i=1}^n ({\mat{Z}}_i - \bar{\mat{Z}})({\mat{Z}}_i - \bar{\mat{Z}})^T.
$$

We know that $\text{rank}(\hat{\mat{R}}) = \text{min}(p,~n - 1)$, which we will denote as $r$.
Since $\hat{\mat{R}}$ is a symmetric and real, it is guaranteed to have an eigendecomposition.
We can therefore diagonalize $\hat{\mat{R}}$ as

$$
\hat{\mat{R}} = \mat{V} \mat{\Lambda} \mat{V}^{-1},
$$

where $\mat{\Lambda} = \text{diag}(\lambda_1, ..., \lambda_r, 0, ..., 0) \in \mathbb{R}^{p \times p}$ is the diagonal eigenvalue matrix sorted in \textit{decreasing} order and $\mat{V} = [\vec{v}_1, ..., \vec{v}_p]$ is a matrix which columns consists of the respective \textit{eigenvectors} of $\hat{\mat R}$.
It can be shown that $\mat{\phi} = [\phi_1, ..., \phi_p]^T = \mat{V}$ solves equation \eqref{eq:decomposition}.

The *principal component scores* are the new values we attain in this transformed space.
The transformation is given as $\mat{Z} \mat{\phi} = \mat{Z} \mat{V} \in \mathbb{R}^{p \times p}$, where only the $r$ first columns are nonzero.
We therefore end up in practice with $r$ principal component scores for each of the original observations.

**Q22:** Explain what is given in the plot with title "First eigenvector". Why are there only $n=64$ eigenvectors and $n=64$ principal component scores?

```{r, fig.cap = "\\label{fig:eigenvector} First eigenvector."}
library(ElemStatLearn)
X <- t(nci)  # n times p matrix
table(rownames(X))

ngroups <- length(table(rownames(X)))
cols <- rainbow(ngroups)
cols[c(4, 5, 7, 8, 14)] <- "black"
pch.vec <- rep(4, 14)
pch.vec[c(4, 5, 7, 8, 14)] <- 15:19

colsvsnames <- cbind(cols, sort(unique(rownames(X))))
colsamples <- cols[match(rownames(X), colsvsnames[,2])] 
pchvsnames <- cbind(pch.vec, sort(unique(rownames(X))))
pchsamples <- pch.vec[match(rownames(X), pchvsnames[,2])]

Z <- scale(X)
pca <- prcomp(Z)
plot(1:dim(X)[2], pca$rotation[,1], type="l", xlab="genes", ylab="weight")
```

Figure \ref{fig:eigenvector} shows the values of of the first eigenvector of $\hat{\mat{R}}$.
With other words, it shows the entries of $\phi_1$, how to linearly combine the columns of $\mat{Z}$ in order to get the first principal component, which explains the most variance of all the principal components.

For our dataset, we have $r = \text{min}(p,~n - 1) = \text{min}(6830,~64 - 1) = 63$.
There should therefore be $n = 63$ eigenvectors, and $n = 63$ principal component scores for each observation.
The \texttt{R}-function `prcomp()` returns 64, but the last eigenvector is the first of the $n - r$ eigenvectors which are spanned by $\text{Null}(Z)$.
We would therefore expect $\lambda_{64} = 0$, which we can check

```{r}
pca$sdev[64]^2
```

This is for all intents and purposes equal to $0$, which is what we expected.

**Q23:** How many principal components are needed to explain 80% of the total variance in ${\mat{Z}}$? Why is `sum(pca$sdev^2)=p`? 

If we transform $\mat{Z}$ into the first $M$ principal components, the fraction of the original variance that is kept is

$$
  R^2
  =
  \frac{
    \sum_{i = 1}^M \lambda_i
  }
  {
    \sum_{j = 1}^p \lambda_j
  }
$$

We can calculate these values by retrieving `pca$sdev`

```{r}
library(magrittr)  # For is_weakly_greater_than()
total_variance <- sum(pca$sdev^2)
sprintf("Total variance: %.0f", total_variance)

cummulative_variance <- cumsum(pca$sdev^2)
proportional_variance <- cummulative_variance / total_variance
sufficient_components <- proportional_variance %>%
  is_weakly_greater_than(0.8) %>%
  which() %>%
  first()
sprintf(
  "80 percent of variance is explained by the first %.0f principal components",
  sufficient_components
)
```

We can explain 80\% of the variance with the 32 first principal components.
Since the principal component analysis is performed on the *standardized* covariance matrix where the variance for each covariate has been scaled to $1$ the total variance becomes $\sum_{i = 1}^p 1 = p$.


**Q24**: Study the PC1 vs PC2 plot, and comment on the groupings observed. What can you say about the placement of the `K262`, `MCF7` and `UNKNOWN` samples? Produce the same plot for two other pairs of PCs and comment on your observations.

```{r, fig.cap = "\\label{fig:pc1vs2}PC1 scores plotted against PC2 scores"}
plot_pca <- function(first, second) {
  plot(
    pca$x[,first],
    pca$x[,second],
    xlab = paste("PC", first, sep = ""),
    ylab = paste("PC", second, sep = ""),
    pch = pchsamples,
    col = colsamples
  )
  legend(
    "bottomright",
    legend = colsvsnames[,2],
    cex = 0.55,
    col = cols,
    pch = pch.vec
  )
}

plot_pca(1, 2)
```

We can see in \ref{fig:pc1vs2} the first principal component scores plotted against the second.
`MELANOMA` cancer cells clustered away from the other cancer tumors with respect to the `PC2`-axis.
`LEUKEMIA`, on the other hand, is separated away from most of the others along the `PC1`-axis.
The `K562`, which are known as Leukemia cells, are clustered in the vicinity of `COLON` and `LEUKEMIA` which may imply a sort of similarity between these cell types.
The same can be said of `MCF7`, which are known to be breast cancer cells.
The `UNKNOWN` sample seems to be clustered close to `PROSTATE` and `NSCLC`/`OVARIAN` (unsure about the last one, because the colors are so similar). Perhaps `UNKNOWN` is an `NSCLC` sample?

We will now plot `PC3` against `PC4`.

```{r}
plot_pca(3, 4)
```

Here we see less distinct clusters. `LEUKEMIA` and `COLOR` are somewhat separated from the rest. The `UNKNOWN` sample is close to `CNS`, amongst others. Most of the points are placed close to the line $\texttt{PC1} = \texttt{PC2}$.

An lastly, we plot `PC63` against `PC64`.

```{r}
plot_pca(63, 64)
```

Here it becomes really apparent that $\mat{W}_{64} \in \text{Null}(Z)$ as all the observations are placed along the line $\texttt{PC64} = 0$ within numerical error.


**Q25:**: Explain what it means to use Euclidean distance and average linkage for hierarchical clustering. 

A cluster, $C_k$, is considered to be good if it has a *within-cluster variation* that is as small as possible.
The within-cluster variation, $W(C_k)$, is defined in terms of the *squared* Euclidean distance between the members of the cluster

$$
W(C_k) = \frac{1}{|C_k|} \sum_{i, i' \in C_k} \sum_{j = 1}^p (x_{ij} - x_{i'j})^2,
$$

where $|C_k|$ is the cardinality of the cluster set.
Thus we want to find a cluster superset $C = \{C_1, ..., C_k\}$ which minimizes $\sum_{k = 1}^K W(C_k)$.
Several algorithms exists for finding such cluster sets, one of which is the *hierarchical clustering algorithm*.
The algorithm goes as follows:

\begin{enumerate}
  \item Start with all $n$ observations, i.e. the bottom of the \textit{dendogram} (see next task). Consider each observation to be its own cluster.
  \item \texttt{while number of clusters} $> 1$:
  \begin{enumerate}
    \item Fuse the two clusters that are the most similar to each other.
    \item Decrement \texttt{number of clusters} by $1$.
  \end{enumerate}
\end{enumerate}

A question remains to be answered; how to define *similarity* between clusters?
Consider two clusters consisting of a single observation each, $\vec{x}_1$ and $\vec{x}_2$.
We can use the *euclidean distance* $||\vec{x}_1 - \vec{x}_2||_2$ as a similarity measure amongst others.

But how do we measure the similarity between two clusters with cardinalities that are not equal to $1$?
We need to generalize the concept of similarity to groups of observations.
This is problem called *linkage*.
There are no straight forward answers here, but one proposal solution is to use *mean inter-cluster dissimilarity*, aka. "average linking".
Define the average distance measure $D_a$ between two clusters $C_1$ and $C_2$ as

$$
D_{a}(C_1, C_2)
=
  \frac{1}{|C_1| \cdot |C_2|}
  \sum_{x_i, x_j \in C_1 \times C_2} ||x_i - x_j||_2,
$$

where $C_1 \times C_2$ is the Cartesian product between the two cluster sets.
This is now what we want to minimize when we fuse two clusters of arbitrary cardinality.

**Q26:**: Perform hierarchical clustering with Euclidean distance and average linkage on the scaled gene expression in `Z`. Observe where our samples labelled as K562, MCF7 and `UNKNOWN` are placed in the dendrogram. Which conclusions can you draw from this?

```{r, fig.height = 5, fig.fullwidt = TRUE}
dissimilarity_structure <- dist(Z, method = "euclidean")
hc_model <- hclust(dissimilarity_structure, method = "average")
plot(hc_model, cex = 0.45)
```

The two `K562` cells are first clustered together, and afterwards with all the other `LEUKEMIA` samples. This is what we would expect, as they should be similar.
The `MCF7` cells, which are known to be breast cancer cells, are also clustered with `BREAST` cells, which is also expected.

With other words, cancerous cells show a degree of similarity to the surrounding cells in the body.

Lastly, the `UNKNOWN` cell is considered to be relatively close to an `OVARIAN` sample.
Perhaps this cell sample is ovarian, possibly cancerous?

**Q27:**: Study the R-code and plot below. Here we have performed hierarchical clustering based on the first 64 principal component instead of the gene expression data in `Z`. What is the difference between using all the gene expression data and using the first 64 principal components in the clustering? We have plotted the dendrogram together with a heatmap of the data. Explain what is shown in the heatmap. What is given on the horizontal axis, vertical axis, value in the pixel grid?

In task **Q26** observations are clustered according to their respective euclidean distances in the original covariate vector space $\mathbb{R}^p$, i.e.

$$
|| \vec{z}_1 - \vec{z}_2 ||_2
=
\sqrt{ \sum_{j = 1}^{p} (z_{1j} - z_{2j})^2 }
$$

When performing hierarchical clustering on the principal component scores, the distance measure is defined over the principal component space $\mathbb{R}^r$ instead.

$$
  || \vec{w}_1 - \vec{w}_2 ||_2
  =
  \sqrt{ \sum_{j = 1}^{r} \left(w_{1j} - w_{2j}\right)^2 }
$$

The $64^{\text{th}}$ principal component score will always be $\simeq 0$, so it will not contribute to the distance.
Which different does this make?
Observe the following

\begin{gather*}
  || \vec{w}_1 - \vec{w}_2 ||_2
  =
  || \vec{z}_1^T \mat{V} - \vec{z}_2^T \mat{V} ||_2
  =
  || (\vec{z}_1 - \vec{z}_2)^T \mat{V} ||_2 \\
  %
  \leq
  || \vec{z}_1 - \vec{z}_2 ||_2 ||\mat{V}||_2
  =
  || \vec{z}_1 - \vec{z}_2 ||_2
\end{gather*}

Here we have used that the columns of the eigenvector matrix $\mat{V}$ are *orthonormal* which results in $||\mat{V}||_2 = 1$.
We can see that the distances have been contracted in the principal component space.

Why is this instructive?
Consider a data set with two covariates, $x_1$ and $x_2$, where the two covariates are almost *perfectly* correlated, let's say $x_{i2} = x_{i1} + \epsilon_i$.
The original distance measure between $x_1 = (1, 1)$ and $x_2 = (2, 2)$ would become $\sqrt{2}$, while the two respective principal components would have a distance of $\approx 1$.
The clustering of the principal components therefore accounts for correlations which may contribute to overestimations of the dissimilarity between observations!

We now plot the principal component scores as a heatmap.

```{r}
library(pheatmap)
par(mfcol=c(1, 1))
npcs <- 64 
pheatmap(
  pca$x[,1:npcs],
  scale = "none",
  cluster_col = FALSE,
  cluster_row = TRUE,
  clustering_distance_rows = "euclidean",
  clustering_method = "average",
  fontsize_row = 5,
  fontsize_col = 5
)
```

The hierarchical clustering algorithm can be seen on the left vertical axis.
The horizontal axis shows the principal components, while the right vertical axis shows the observations.
The pixels show the principal component scores of each observation, red pixels indicate great positive scores while blue values indicate great negative scores.
Here it becomes apparent that the first principal component has the most variance, and that the variance decreases as we move right.
The last column, which is the 64th principal components, is clearly populated by zeros, as explained earlier.

# Problem 3: Flying solo with diabetes data [6 points]

First we import the data set.

```{r}
flying <- dget("flying.dd")
ctrain <- flying$ctrain %>% as_tibble()
ctest <- flying$ctest %>% as_tibble()
```

**Q28:** Start by getting to know the _training data_, by producing summaries and plots. Write a few sentences about what you observe and include your top 3 informative plots and/or outputs.

### `summary()`

First it is practical to get a quick summary of the data structure of the training set by using `summary()`:

```{r}
summary(ctrain)
```

First of, we observe that *all* the columns are of numeric types.
This is okay for the covariates, as they are considered to be numeric, but we prefer to encode the `diabetes` response as a factor variable, so we will do this straight away.

```{r}
ctrain$diabetes <- as.factor(ctrain$diabetes)
ctest$diabetes <- as.factor(ctest$diabetes)
```

Otherwise, we can see that one third of the observations have `diabetes = 1`.
Also, the median number of pregnancies is $2$ and the average is $3.437$, so we have a positive skew in pregnancies.
The `age` column spans from $21$ to $70$, which is good, since most age segments are represented.
Lastly, the average and median `bmi` is $\approx 33$, which is considered to be obese, which makes sense in a sample set where $33\%$ of the observations have diabetes.

### `ggpairs()`

Next up we can use `ggpairs()` to get a really quick overview of the distributional properties of the data and how they correlate.

```{r, cache = TRUE}
library(GGally)
ggpairs(
  ctrain,
  mapping = aes(color = diabetes, alpha = 0.5),
  upper = list(continuous = wrap("cor", size = 2.5)),
  lower = list(continuous = wrap("points", size = 0.2))
)
```

Here, `diabetes = 0` is shown in red and `diabetes = 1` is shown in blue.
We can now see that diabetes sufferers are overrepresented in the right tail of all the covariates (see the diagonal), especially `glu`.

### `ggplot()`

A priori, we would predict glucose levels and body mass index to be positively correlated, so we can take an extra closer look at this.

```{r}
ctrain %>%
  ggplot() +
  aes(x = glu, y = bmi, color = diabetes) +
  geom_point()
```

Here we see `glu` plotted against `bmi`, where the blue color represents `diabetes` sufferers.
Again, diabetes is highly overrepresented in the upper right corner, so this confirms the positive correlation.

**Q29:** Use different methods to analyse the data. In particular use 

* one method from Module 4: Classification

```{r}
library(magrittr)  # For use_series()
library(caret)  # For confusionMatrix()
library(e1071)  # For ConfusionMatrix()
library(purrr)  # For map_dbl()

# Create bestglm-compliant dataframe
Xy <- as.data.frame(
  within(
    ctrain,
    {
      y = diabetes    # setting response
      diabetes = NULL # not include as covariate
    }
  )
)

# Logistic model selection based on AIC
logistic_model <- bestglm(
    Xy = Xy,
    family = binomial,
    IC = "AIC"
  ) %>%
  use_series(BestModel)

# Create diabetes predictions
logistic_predictions <- predict(logistic_model, ctest, type = "response") %>%
  map_dbl(~ .x > 0.5) %>%
  as.factor()

# Create confusion matrix
logistic_confmat <- confusionMatrix(
  data = logistic_predictions,
  reference = ctest$diabetes %>% as.factor()
)
print(logistic_confmat)
```

* one method from Module 8: Trees (and forests)

```{r, eval = FALSE}
library(xgboost)
xgb_train_data <- xgb.DMatrix(
  as.matrix(ctrain %>% dplyr::select(-diabetes)),
  label=ctrain$diabetes
)
xgb_model <- xgboost(
  data = xgb_train_data,
  nrounds = 10,
  objective = "binary:logistic"
)

xgb_test_data <- xgb.DMatrix(
  as.matrix(ctest %>% dplyr::select(-diabetes)),
  label=ctest$diabetes
)

xgb_predictions <- predict(
    xgb_model,
    newdata = xgb_test_data,
    type = "response"
  ) %>%
  map_dbl(~ .x > 0.5) %>%
  as.factor()

xgb_confmat <- confusionMatrix(
  data = xgb_predictions,
  reference = ctest$diabetes %>% as.factor()
)

print(xgb_confmat)

importance_frame <- xgb.importance(colnames(xgb_train_data), model = xgb_model)
xgb.plot.importance(importance_frame)
```

* one method from Module 9: Support vector machines and, finally
* one method from Module 11: Neural networks

For each method you 

* clearly write out the model and model assumptions for the method
* explain how any tuning parameters are chosen or model selection is performed
* report (any) insight into the interpretation of the fitted model
* evaluate the model using the test data, and report misclassification rate (cut-off 0.5 on probability) and plot ROC-curves and give the AUC (for method where class probabilities are given).

**Q30:** Conclude with choosing a winning method, and explain why you mean that this is the winning method.

