---
subtitle: "TMA4268 Statistical Learning V2019"
title: "Compulsory exercise 2: Group XYZ"
author: "Martinussen, Jakob Gerhard & Wilson, Alm"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    latex_engine: xelatex
    keep_tex: true
header-includes:
  - \usepackage{fontspec}
  - \setmonofont{DejaVu Sans Code}
  - \makeatletter
  - \def\verbatim@nolig@list{}
  - \makeatother
  - \newcommand{\mat}[1]{\boldsymbol{#1}}

  - \usepackage{bm}
  - \usepackage[a]{esvect}
  - \renewcommand{\vec}[1]{\vv{\bm{#1}}}
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error=FALSE)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(results="hold")
```

# Problem 1: Regression [6 points]

```{r}
all <- dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/diamond.dd")
dtrain <- all$dtrain
dtest <- all$dtest
```

**Q1**: Would you choose `price` or `logprice` as response variable? Justify your choice. Next, plot your choice of response pairwise with `carat`, `logcarat`, `color`, `clarity` and `cut`. Comment.

We choose `logprice` as the response variable, since we postulate that diamond qualities have a *multiplicative* effect on the price, not *additive*. A logarithmic response will result in such an interpretation of the regression coefficients. We now create the plots as instructed.

```{r}
library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)

# Combine both training and testing set for plotting
dall <- bind_rows(dtrain, dtest)

# Plot categorical covariates
dall %>%
  gather(key, value, c("cut", "color", "clarity")) %>%
  ggplot(aes(y = logprice)) +
  geom_point(aes(x = value), alpha = 0.3) +
  facet_grid(.~key, scales = "free")

# Plot continuous covariates
dall %>%
  gather(key, value, c("carat", "logcarat")) %>%
  ggplot(aes(y = logprice)) +
  geom_point(aes(x = value)) +
  facet_grid(.~key, scales = "free")
```

We observe that `logcarat` seems to be the covariate that has the most linear effect, which is promising. The `clarity`, `color`, and `cut` covariates do not seem to have little to none distinguishing effect on the price.

**Q2:** What is the predicted price of a diamond weighting 1 carat. Use the closest 20% of the observations. 

We now use the local regression model $\texttt{logprice} = \beta_0 + \beta_1 \texttt{carat}+ \beta_2 \texttt{carat}^2$ weighted by the tricube kernel $K_{i0}$.

```{r}
model1 <- loess(logprice ~ carat + carat^2, data = dtrain, span = 0.2)
```

Why can't we use `as.is`?

We make a price prediction for a 1 carat diamond, using the closest 20\% of the observations:

```{r}
new_observation <- list(carat = 1)
price_prediction <- 10 ^ predict(model1, new_observation)
sprintf("$%.2f USD", price_prediction)
```

So the prediction is that a 1 carat diamond will cost $\simeq$ $5100 USD.

**Q3:** What choice of $\beta_1$, $\beta_2$ and $K_{i0}$ would result in KNN-regression?

If we set the following values

$$
\beta_1 \leftarrow 0 \\
\beta_2 \leftarrow 0 \\
K_{i0} \leftarrow \frac{1}{K} I(i \in \mathcal{N}_0), 
$$

it yields the following

$$
\sum_{i = 1}^n K_{i0}(y_i - \beta_1 x_i - \beta_2x_i^2)^2
=
\frac{1}{K} \sum_{i \in \mathcal{N}_0} y_i,
$$

which is the definition of KNN-regression. Here $\mathcal{N}_0$ is the index set for the $K$ nearest neighbours to $x_0$, measured in Euclidean distance.

**Q4:** Describe how you can perform model selection in regression with AIC as criterion.

The Akaike Information Criterion (AIC) is defined as

$$
\text{AIC} := -2 \cdot l(\hat{\beta}_M, \tilde{\sigma}^2) +2(\lvert M\rvert +1),
$$

where $\hat{\beta}_M$ is the estimated coefficient vector for a model with $M$ regression parameters, and $l(\cdot)$ is the logarithm of the maximum likelihood estimator for that model.
How you calculate the maximum likelihood depends on what type of regression you perform.
We use $|M| + 1$, because we have to add the estimated variance of the error terms $\epsilon_i$.

When we do model selection, i.e. picking between different candidate models, we can pick the model with the *lowest* value for AIC.

**Q5:** What are the main differences between using AIC for model selection and using cross-validation (with mean squared test error MSE)?

Here are the main differences:

* CV requires you to split your data set into *training* and *testing* subsets. This results in smaller sample sizes for fitting the models, and can result in an overestimation of the validation set error.
* CV test MSE can have high variability for smaller sample sizes, since it is dependent on which observations are included/excluded.

**Q6:** See the code below for performing model selection with `bestglm()` based on AIC. What kind of contrast is used to represent `cut`, `color` and `clarity`? Write down the final best model and explain what you can interpret from the model.

```{r}
library(bestglm)
ds <- as.data.frame(
  within(
    dtrain,
    {
      y = logprice    # setting response
      logprice = NULL # not include as covariate
      price = NULL    # not include as covariate
      carat = NULL    # not include as covariate
    }
  )
)

fit <- bestglm(Xy=ds, IC="AIC")$BestModel
summary(fit)
```

From the summary we can see that `cut`, `color`, and `clarity` are *dummy coded* with respectively `cutFair`, `colorD`, and `clarityI1` as reference categories.

The final best model is `logprice ~ 1 + logcarat cut + color + clarity + xx`, or in mathematical form

$$
\texttt{logprice}_i
= \\
  \beta_0 \\
  + \beta_1 \cdot x_{i, \texttt{logcarat}} \\
  + \beta_2 \cdot x_{i, \texttt{cutGood}} + ... + \beta_5 \cdot x_{i, \texttt{cutIdeal}} \\
  + \beta_6 \cdot x_{i, \texttt{colorE}} + ... + \beta_{11} \cdot x_{i, \texttt{colorJ}} \\
  + \beta_{12} \cdot x_{i, \texttt{claritySI2}} + ... + \beta_{18} \cdot x_{i, \texttt{clarityIF}} \\
  + \beta_{19} \cdot x_{\texttt{xx}} \\
  + \epsilon_i.
$$

A `price` prediction for the covariate vector $\vec{x}_0$ is constructed in the following manner

$$
\widehat{\texttt{logprice}}
=
\vec{\hat{\beta}}^T \vec{x}_0 \\
\implies
\widehat{\texttt{price}}
=
10^{\vec{\hat{\beta}}^T \vec{x}_0} = 10^{\beta_0} \cdot \prod_{i = 1}^{19} 10^{\beta_i \vec{x}_{0,i}}.
$$

The tenth power of the regression coefficients are therefore of interest.

```{r}
library(kableExtra)
beta_hat <- coefficients(fit)
10^beta_hat %>% kable(col.names = "$10^{\\hat{\\beta}_i}$")
```

Several things can be interpreted from this. The `(Intercept)` represents a 1 carat diamond, i.e. 0 `logcarat`, with a *fair* cut, *D* color category, and *I1* clarity category. Such a diamond is predicted to cost $\simeq$ \$968 USD.
All other covariates must be interpreted *relative to* this "reference diamond". For instance, since $10^{\vec{\hat{\beta}}_{\texttt{cutIdeal}}} \approx 1.135$, we would expect a diamond with an *ideal cut* to cost 13.5\% more than the "reference diamond", all other properties being equal.

**Q7:** Calculate and report the MSE of the test set using the best model (on the scale of `logprice`).

```{r}
best_subset_test_mse <- mean((dtest$logprice - predict.lm(fit, dtest))^2)
print(best_subset_test_mse)
```

The test MSE is $\simeq 0.0036$.


**Q8:** Build a model matrix for the covariates `~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1`. What is the dimension of this matrix?

We build the design matrix by using the `model.matrix()` function.

```{r}
lasso_formula <- ~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1
model_matrix <- model.matrix(
  lasso_formula,
  data = dtrain
)
```

Since we use dummy coding of categorical covariates, each covariate with $c$ categories result in $c - 1$ columns in the design matrix.
But in this case, we have no intercept, so one of the categorical covariates will have $c$ columns, one for each covariate.
We have 3 categorical variables, with 5, 8, and 7 categories. This results in 17 columns, instead of 16 if we had an intercept in our model.
In addition, we have 6 continuous covariates.
All together this results in 24 columns in the design matrix and $n$ rows, where $n$ equals the number of observations, in this case $5000$.
To confirm, we can check

```{r}
dim(model_matrix)
```

This coincides.


**Q9:** Fit a lasso regression to the diamond data with `logprice` as the response and the model matrix given in Q8. How did you find the value to be used for the regularization parameter?

We will fit a lasso regression model to the design matrix from **Q8**.
In order to choose a suitable value for the tuning parameter $\lambda$, we use 10-fold cross validation and the 1 standard deviation rule as explained in exercise 1. We let `glmnet` choose a suitable range of $\lambda$ values for us.


```{r}
library(glmnet)
cv.out <- cv.glmnet(
  x = model_matrix,
  y = dtrain$logprice,
  alpha=1,          # Lasso
  nfolds=10,        # 10-fold CV
  standardize=TRUE  # Standardize coefficients, see: https://think-lab.github.io/d/205/#5
) 
```

The cross validation MSE can be plotted against $\lambda$.

```{r}
plot(cv.out)
```

The two vertical lines correspond to the $\lambda$ with the least CV MSE (left) and the $\lambda$ chosen with the 1SD-rule (right), with the specific values

```{r}
print(cv.out$lambda.min)
print(cv.out$lambda.1se)
```

The chosen regularization parameter is therefore $\lambda \approx 5.2 \cdot 10^{-4}$, which is within one standard deviation of $\lambda_{\text{min}} \approx 1.2 \cdot 10^{-4}$.

**Q10:** Calculate and report the MSE of the test set (on the scale of `logprice`).

We can now calculate the test MSE.

```{r}
lasso_model <- cv.out$glmnet.fit
chosen_lambda <- cv.out$lambda.1se
xtest <- model.matrix(lasso_formula, data = dtest)
predictions <- predict(lasso_model, s = chosen_lambda, newx = xtest)
lasso_test_mse <- mean((dtest$logprice - predictions) ^ 2)
print(lasso_test_mse)
```

The test MSE is $\approx 3.88 \cdot 10^{-3}$ on the scale of `logprice`.


**Q11:** A regression tree to model is built using a _greedy_ approach. What does that mean? Explain the strategy used for constructing a regression tree.

A _greedy_ approach means that the apparently best choice is taken at any one time, not considering the possibility of a suboptimal choice may be taken in order to achieve an *overall* better result further down the road. Once a decision has been made, no backtracking is performed, and the choice is "locked in".

The idea of a regression tree is to take $n$ observation pairs $(\vec{x}_i, Y_i)$ and separating them into $J$ partitions of the predictor space, $R_1, ..., R_J$.
Predictions are made by identifying the region which a predictor falls within and then taking the average response of all the pairs of that region.

A prediction, $\hat{f}_{R_j}$, for $\vec{x}_0$ belonging to the region $R_j$, is therefore calculated as

$$
\hat{f}_{R_j} = \frac{1}{|R_j|} \sum_{i: x_i \in R_j} y_i,
$$

where $|R_j|$ is the number of observed pairs placed within region $R_j$.

Now a question remains; how to construct these regions? We use a greedy approach called *recursive binary splitting*. Define the parametrized regions

\begin{gather*}
  R_1(j, s) := \{\vec{x}~|~x_j < s\}, \\
  R_2(j, s) := \{\vec{x}~|~x_j \geq s\}.
\end{gather*}

We want to pick the predictor $j$ and split-off point $s$ that minimizes the sum of the residual sum of squares of both sub-partition, i.e.

$$
(j, s)
=
\underset{(j, s)}{\text{argmin}}
\left\{
\sum_{i: x_i \in R_1(j, s)} (y_i - \hat{f}_{R_1(j, s)})^2
+
\sum_{i: x_i \in R_2(j, s)} (y_i - \hat{f}_{R_2(j, s)})^2
\right\}
$$

We choose $(j, s)$ recursively without backtracking (thus greedy), until some stopping criterion is reached.
The stopping criterion can the number of members of the resulting region being small enough, or when the reduction in RSS is smaller than some threshold.

**Q12:** Is a regression tree a suitable method to handle both numerical and categorical covariates? Elaborate.

Yes, regression trees can handle mixed features, which is one of its advantages.
Instead of deciding node traversal by a "less than" operator, as in the numerical case, a decision can be made by a "containment" operator in the categorical case. For instance if $x_j$ is a categorical covariate representing if a patient is smoking (coded as $1$) or not (coded as $0$), a splitting criterion could be

\begin{gather*}
  R_1 = \{ \vec{x}~|~x_j = 1 \} \\
  R_2 = \{ \vec{x}~|~x_j = 0 \}.
\end{gather*}

**Q13:** Fit a (full) regression tree to the diamond data with `logprice` as the response (and the same covariates as for c and d), and plot the result. Comment briefly on you findings.

We now fit a full regression tree for the diamond data.

```{r}
library(tree)
diamond_formula <- logprice~logcarat+cut+clarity+color+depth+table+xx+yy+zz-1
full_tree <- tree(
  diamond_formula,
  data = dtrain
)
```

The decision tree can be visualized together with the partition space.
We will transform the `logprice` to `price` for easier interpretability.

```{r}
# For deep copy of data-structures
library(rlang)

# Plot side-by-side
par(mfcol = c(1, 2))

# Convert fitted values from logprice to price
plot_tree <- duplicate(full_tree, shallow = FALSE)
plot_tree$frame$yval <- 10 ^ plot_tree$frame$yval

# Plot the decision tree together with decision criteria
plot(plot_tree, type = "proportional")
text(plot_tree, pretty = 1, cex = 0.5)

# Plot the partitioning of the xx-yy plane
partition.tree(plot_tree, cex = 0.4)
```

The length of the edges in the tree on the left hand side are proportional to the reduction in RSS.
We can see that the first split, $\texttt{yy} < 5.665$, by far reduces the RSS the most. The tree algorithm also decides that *above* this split, only $\texttt{yy}$ matters, and $\texttt{xx}$ is ignored. The greater $\texttt{xx}$ and $\texttt{yy}$ is, the greater the price prediction.
Lastly, it is interesting that solely $\texttt{xx}$ and $\texttt{yy}$ are chosen, all of the other covariates have been discarded. Using this model as a human is quite easy, as there are only two covariates to consider.


**Q14:** Calculate and report the MSE of the test set (on the scale of `logprice`).

We can now calculate the test MSE on the scale of `logprice`.

```{r}
tree_predictions <- predict(full_tree, dtest)
tree_test_mse <- mean((tree_predictions - dtest$logprice)^2)
print(tree_test_mse)
```

The test MSE is $\approx 1.72 \cdot 10^{-2}$, a whole order of magnitude greater than the lasso model fitted earlier. It should be noted, though, that this model is *substantially* less complex than the lasso model.


**Q15:** Explain the motivation behind bagging, and how bagging differs from random forest? What is the role of bootstrapping?

A problem with decision trees is that they often portray a large amount of variance, since small changes in the predictors will often result in very different trees. One solution to this problem is *bootstrap aggregation*, more commonly known as *bagging*.

The idea is to draw a sufficiently large amount, $B$, bootstrap replicates from the training set and fit a tree, $\hat{f}_{b}(\vec{x})$, for every single replicate, $X_b$.
The resulting predictor, $\hat{f}_{\text{avg}}(\vec{x})$ is the *average* of all those trees,

$$
\hat{f}_{\text{avg}}(\vec{x}) = \frac{1}{B} \sum_{b = 1}^{B} \hat{f}_b (\vec{x}).
$$

The motivation is this, assume that we have $B$ i.i.d. observations of the random variable $X$ with mean $\mu$ and variance $\sigma^2$. The unbiased mean estimator, $\bar{X} = \frac{1}{B} \sum_{b = 1}^B X_b$, has a variance of

$$
Var(\bar{X})
=
\mathrm{Var}\left( \frac{1}{B} \sum_{b = 1}^B X_b \right)
=
\frac{1}{B^2} \sum_{b = 1}^B \mathrm{Var}(X_b) = \frac{\sigma^2}{B}
$$

The variance of the estimator is thus inversely proportional to the number of independent samples $B$. This effect can be *simulated* by generating $B$ bootstrap data sets; they are not completely independent but is better than nothing.

Bagging is not the end all solution, however. If there is a really strong predictor in the training set, then this predictor will end up being used in the first split in all the bootstrap replicated trees.
This results in the trees becoming highly correlated, and we lose much of the variance reduction which we expect from the averaging of the trees. A *random forest* aims to solve this by randomly removing a fraction of the covariates in each split, instead of using *all* the covariates as we do in *bagging*. This is intended to decorrelates the trees.

**Q16:** What are the parameter(s) to be set in random forest, and what are the rules for setting these?

There are two parameters of interest when creating a *random forest*.
The first is the number of trees, $B$. This is *not* a tuning parameter, however, and the larger the better. Pick a value that is sufficiently large while being computationally feasible.

The other parameter is $m$, the number of covariates to be randomly kept in each split. First off, we require $m < p$, of course. For very correlated predictors, $m$ is chosen to be small.
A good *a priori* rule-of-thumb is to choose $m \approx \sqrt{p}$ for classification and $m = p / 3$ for regression. This has been derived from simulations.

**Q17:** Boosting is a popular method. What is the main difference between random forest and boosting?

A random forest is created by averaging $B$ trees fitted in parallel with the use of bootstrapping. The trees not "care about" each other.

Boosting is different in that it is an *iterative* or *sequential* method. It first builds a decision tree with $d$ splits, and calculates the residuals of this tree.
After this, a *new* tree is fitted on the residuals. The original tree is updated by adding a shrunken version of the new tree. This is repeated $B$ times, always fitting new trees on the new residuals.

**Q18:** Fit a random forest to the diamond data with `logprice` as the response (and the same covariates as before). Comment on your choice of parameter (as decribed in Q16).

We fit a random forest with the same formula.

```{r, include = FALSE}
library(randomForest)
```

```{r, cache = TRUE}
random_forest <- randomForest(
  diamond_formula,
  data = dtrain,
  mtry = dim(model_matrix)[2] / 3,
  importance = TRUE
)
```

We have here chosen `mtry = 24 / 3 = 8` since we have a continuous response and this is a regression tree. With other words, we are using the rule-of-thumb $m = p / 3$.

**Q19:** Make a variable importance plot and comment on the plot. Calculate and report the MSE of the test set (on the scale of `logprice`).

In order to interpret this *ensemble method* we can make a variable importance plot.

```{r}
varImpPlot(random_forest)
```

We can see that `yy` is decidedly best at decreasing the total node impurity, while `clarity` and `color` are best when using the alternative random out-of-bag importance method. There is no overlap between the 2 best covariates in each method.

We can now calculate the MSE for the test set on the scale of `logprice`.

```{r}
random_forest_predictions <- predict(random_forest, dtest)
random_forest_test_mse <- mean((random_forest_predictions - dtest$logprice)^2)
print(random_forest_test_mse)
```

We have a test MSE of $\approx 2.36 \cdot 10^{-3}$.


**Q20:** Finally, compare the results from c (subset selection), d (lasso), e (tree) and f (random forest): Which method has given the best performance on the test set and which method has given you the best insight into the relationship between the price and the covariates?

Let $\mat{X}$

We can now compare the test MSE scores of the four previous models.

```{r}
test_mse <- tibble(
  best_subset = best_subset_test_mse,
  lasso = lasso_test_mse,
  tree = tree_test_mse,
  random_forest = random_forest_test_mse
)
t(test_mse) %>% kable(col.names = c("Test MSE"))
```

We can see that the random forest has performed the best on the test set.
The random forest is difficult to interpret though, especially since the two importance plots do not coincide.
The tree model is much simpler to interpret, it condenses all the information down to just two covariates, `xx` and `yy`, making it easy to print out on a piece of paper and taking with you to the Jeweler.


# Problem 2: Unsupervised learning [3 points]

**Q21:** What is the definition of a principal component score, and how is the score related to the eigenvectors of the matrix \({\hat {\mat R}}\). 

Assume that you have an $n \times p$ design matrix $\mat{X}$, with a standardized version $\mat{Z}$ where the columns have been centered (having mean $0$) and scaled (having variance $1$).
We want to linearly transform the set of (possibly) correlated covariates into a new orthogonal basis where the *new* variables are linearly uncorrelated while retaining as much of the information (variance) in the data as possible.
These new variables, $\mat{W}_m$, are called *principal components* and the problem can be formally defined as trying to solve

\begin{align}
  \label{eq:decomposition}
  &\mat{W}_m
  =
  \hat{\mat{R}} \mat{\phi}_m
  =
  \sum_{j = 1}^p \phi_{jm} \hat{\mat R}_j,
    & \forall m: ||\mat{\phi}_{m}||_{2} &= 1 \nonumber \\
  %
  &\mathrm{Var}(\mat{W}_i) \geq \mathrm{Var}(\mat{W}_{j})
    &\forall i,j: i &< j \\
  %
  &\mat{W}_i \perp \mat{W}_j,
    & \forall i,j: i &\neq j \nonumber.
\end{align}

The new variables are linear combination of the original columns.
Since the components are sorted in decreasing order of variance contributed, we can subset the first $M$ components in order to reduce the dimensionality of the original data set while retaining the maximum amount of information in the data set.

We will now explain how to solve this problem.
The estimated covariance matrix, $\hat{\mat{R}} \in \mathbb{R}^{p \times p}$, for the standardized design matrix, $\mat{Z}$, is given as

$$
{\hat {\mat{R}}}
=
\frac{1}{n-1} \sum_{i=1}^n ({\mat{Z}}_i - \bar{\mat{Z}})({\mat{Z}}_i - \bar{\mat{Z}})^T.
$$

We know that $\text{rank}(\hat{\mat{R}}) = \text{min}(p,~n - 1)$, which we will denote as $r$.
Since $\hat{\mat{R}}$ is a non-negative definite matrix it is guaranteed to have an eigendecomposition, we can rewrite $\hat{\mat{R}}$ as

$$
\hat{\mat{R}} = \mat{V} \mat{\Lambda} \mat{V}^{-1},
$$

where $\mat{\Lambda} = \text{diag}(\lambda_1, ..., \lambda_r, 0, ..., 0) \in \mathbb{R}^{p \times p}$ is the diagonal eigenvalue matrix sorted in \textit{decreasing} order and $\mat{V} = [\vec{v}_1, ..., \vec{v}_p]$ is a matrix which columns consists of the respective \textit{eigenvectors} of $\hat{\mat R}$.
It can be shown that $\mat{\phi} = [\phi_1, ..., \phi_p]^T = \mat{V}$ solves equation \eqref{eq:decomposition}.

The *principal component scores* are the new values we attain in this transformed space.
The transformation is given as $\mat{Z} \mat{\phi} = \mat{Z} \mat{V} \in \mathbb{R}^{p \times p}$, where only the $r$ first columns are nonzero.
We therefore end up in practice with $r$ principal component scores for each of the original observations.

**Q22:** Explain what is given in the plot with title "First eigenvector". Why are there only $n=64$ eigenvectors and $n=64$ principal component scores?

```{r, fig.cap = "\\label{fig:eigenvector} First eigenvector."}
library(ElemStatLearn)
X <- t(nci)  # n times p matrix
table(rownames(X))

ngroups <- length(table(rownames(X)))
cols <- rainbow(ngroups)
cols[c(4, 5, 7, 8, 14)] <- "black"
pch.vec <- rep(4, 14)
pch.vec[c(4, 5, 7, 8, 14)] <- 15:19

colsvsnames <- cbind(cols, sort(unique(rownames(X))))
colsamples <- cols[match(rownames(X), colsvsnames[,2])] 
pchvsnames <- cbind(pch.vec, sort(unique(rownames(X))))
pchsamples <- pch.vec[match(rownames(X), pchvsnames[,2])]

Z <- scale(X)
pca <- prcomp(Z)
plot(1:dim(X)[2], pca$rotation[,1], type="l", xlab="genes", ylab="weight")
```

Figure \ref{fig:eigenvector} shows the values of of the first eigenvector of $\hat{\mat{R}}$.
With other words, it shows the entries of $\phi_1$, how to linearly combine the columns of $\hat{\mat{R}}$ in order to get the first principal component, which explains the most variance of all the principal components.

Since $\mat{X}$ and $\mat{Z}$ are $n \times p = 6380 \times 64$ matrices, there are only $64$ eigenvectors and eigenvalues.

**Q23:** How many principal components are needed to explain 80% of the total variance in ${\mat{Z}}$? Why is `sum(pca$sdev^2)=p`? 

If we transform $\mat{Z}$ into the first $M$ principal components, the fraction of the original variance that is kept is

$$
  R^2
  =
  \frac{
    \sum_{i = 1}^M \lambda_i
  }
  {
    \sum_{j = 1}^p \lambda_j
  }
$$

We can calculate these values by retrieving `pca$sdev`

```{r}
library(magrittr)  # For is_weakly_greater_than()
total_variance <- sum(pca$sdev^2)
sprintf("Total variance: %.0f", total_variance)

cummulative_variance <- cumsum(pca$sdev^2)
proportional_variance <- cummulative_variance / total_variance
sufficient_components <- proportional_variance %>%
  is_weakly_greater_than(0.8) %>%
  which() %>%
  first()
sprintf(
  "80 percent of variance is explained by the first %.0f principal components",
  sufficient_components
)
```

We can explain 80\% of the variance with the 32 first principal components.
Since the principal component analysis is performed on the *standardized* covariance matrix where the variance for each variable has been scaled to $1$ the total variance becomes $\sum_{i = 1}^p 1 = p$.


**Q24**: Study the PC1 vs PC2 plot, and comment on the groupings observed. What can you say about the placement of the `K262`, `MCF7` and `UNKNOWN` samples? Produce the same plot for two other pairs of PCs and comment on your observations.

```{r}
plot_pca <- function(first, second) {
  plot(
    pca$x[,first],
    pca$x[,second],
    xlab = paste("PC", first, sep = ""),
    ylab = paste("PC", second, sep = ""),
    pch = pchsamples,
    col = colsamples
  )
  legend(
    "bottomright",
    legend = colsvsnames[,2],
    cex = 0.55,
    col = cols,
    pch = pch.vec
  )
}

plot_pca(1, 2)
```

**Q25:**: Explain what it means to use Euclidean distance and average linkage for hierarchical clustering. 

**Q26:**: Perform hierarchical clustering with Euclidean distance and average linkage on the scaled gene expression in `Z`. Observe where our samples labelled as K562, MCF7 and `UNKNOWN` are placed in the dendrogram. Which conclusions can you draw from this?

**Q27:**: Study the R-code and plot below. Here we have performed hierarchical clustering based on the first 64 principal component instead of the gene expression data in `Z`. What is the difference between using all the gene expression data and using the first 64 principal components in the clustering? We have plotted the dendrogram together with a heatmap of the data. Explain what is shown in the heatmap. What is given on the horizontal axis, vertical axis, value in the pixel grid?

```{r}
library(pheatmap)
npcs=64 
pheatmap(pca$x[,1:npcs],scale="none",cluster_col=FALSE,cluster_row=TRUE,clustering_distance_rows = "euclidean",clustering_method="average",fontsize_row=5,fontsize_col=5)
```

# Problem 3: Flying solo with diabetes data [6 points]

```{r}
flying <- dget("flying.dd")
ctrain <- flying$ctrain
ctest <- flying$ctest
```

**Q28:** Start by getting to know the _training data_, by producing summaries and plots. Write a few sentences about what you observe and include your top 3 informative plots and/or outputs.

```{r, cache = TRUE}
library(GGally)
ggpairs(ctrain)
```

**Q29:** Use different methods to analyse the data. In particular use 

* one method from Module 4: Classification

```{r}
library(magrittr)  # For use_series()
library(caret)  # For confusionMatrix()
library(e1071)  # For ConfusionMatrix()
library(purrr)  # For map_dbl()

# Create bestglm-compliant dataframe
Xy <- as.data.frame(
  within(
    ctrain,
    {
      y = diabetes    # setting response
      diabetes = NULL # not include as covariate
    }
  )
)

# Logistic model selection based on AIC
logistic_model <- bestglm(
    Xy = Xy,
    family = binomial,
    IC = "AIC"
  ) %>%
  use_series(BestModel)

# Create diabetes predictions
logistic_predictions <- predict(logistic_model, ctest, type = "response") %>%
  map_dbl(~ .x > 0.5) %>%
  as.factor()

# Create confusion matrix
logistic_confmat <- confusionMatrix(
  data = logistic_predictions,
  reference = ctest$diabetes %>% as.factor()
)
print(logistic_confmat)
```

* one method from Module 8: Trees (and forests)

```{r}
library(xgboost)
xgb_train_data <- xgb.DMatrix(
  as.matrix(ctrain %>% dplyr::select(-diabetes)),
  label=ctrain$diabetes
)
xgb_model <- xgboost(
  data = xgb_train_data,
  nrounds = 10,
  objective = "binary:logistic"
)

xgb_test_data <- xgb.DMatrix(
  as.matrix(ctest %>% dplyr::select(-diabetes)),
  label=ctest$diabetes
)

xgb_predictions <- predict(
    xgb_model,
    newdata = xgb_test_data,
    type = "response"
  ) %>%
  map_dbl(~ .x > 0.5) %>%
  as.factor()

xgb_confmat <- confusionMatrix(
  data = xgb_predictions,
  reference = ctest$diabetes %>% as.factor()
)

print(xgb_confmat)

importance_frame <- xgb.importance(colnames(xgb_train_data), model = xgb_model)
xgb.plot.importance(importance_frame)
```

* one method from Module 9: Support vector machines and, finally
* one method from Module 11: Neural networks

For each method you 

* clearly write out the model and model assumptions for the method
* explain how any tuning parameters are chosen or model selection is performed
* report (any) insight into the interpretation of the fitted model
* evaluate the model using the test data, and report misclassification rate (cut-off 0.5 on probability) and plot ROC-curves and give the AUC (for method where class probabilities are given).

**Q30:** Conclude with choosing a winning method, and explain why you mean that this is the winning method.

